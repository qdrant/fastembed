{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤— Huggingface vs âš¡ FastEmbedï¸\n",
    "\n",
    "Comparing the performance of Huggingface's ðŸ¤— Transformers and âš¡ FastEmbedï¸ on a simple task on the following machine: Apple M2 Max, 32 GB RAM\n",
    "\n",
    "## ðŸ“¦ Imports\n",
    "\n",
    "Importing the necessary libraries for this comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:38:48.671752Z",
     "start_time": "2024-03-30T00:38:48.669409Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfastembed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextEmbedding\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModel, AutoTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from fastembed import TextEmbedding\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– Data\n",
    "\n",
    "data is a list of strings, each string is a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:43:34.512097Z",
     "start_time": "2024-03-30T00:43:34.509352Z"
    }
   },
   "outputs": [],
   "source": [
    "documents: List[str] = [\n",
    "    \"Chandrayaan-3 is India's third lunar mission\",\n",
    "    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n",
    "    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n",
    "    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n",
    "    \"The estimated cost of the mission is around $35 million\",\n",
    "    \"It will carry instruments to study the lunar surface and atmosphere\",\n",
    "    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n",
    "    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n",
    "    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n",
    "    \"The mission used GSLV Mk III rocket for its launch\",\n",
    "    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n",
    "    \"Chandrayaan-3 was launched earlier in the year 2023\",\n",
    "]\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up ðŸ¤— Huggingface\n",
    "\n",
    "We'll be using the [Huggingface Transformers](https://huggingface.co/transformers/) with PyTorch library to generate embeddings. We'll be using the same model across both libraries for a fair(er?) comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:43:35.417504Z",
     "start_time": "2024-03-30T00:43:34.800606Z"
    }
   },
   "outputs": [],
   "source": [
    "class HF:\n",
    "    \"\"\"\n",
    "    HuggingFace Transformer implementation of FlagEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_id: str):\n",
    "        self.model = AutoModel.from_pretrained(model_id)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    def embed(self, texts: List[str]):\n",
    "        encoded_input = self.tokenizer(texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        model_output = self.model(**encoded_input)\n",
    "        sentence_embeddings = model_output[0][:, 0]\n",
    "        sentence_embeddings = F.normalize(sentence_embeddings)\n",
    "        return sentence_embeddings\n",
    "\n",
    "\n",
    "model_id = \"BAAI/bge-small-en\"\n",
    "hf = HF(model_id=model_id)\n",
    "hf.embed(documents).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up âš¡ï¸FastEmbed\n",
    "\n",
    "Sorry, don't have a lot to set up here. We'll be using the default model, which is Flag Embedding, same as the Huggingface model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:43:35.486719Z",
     "start_time": "2024-03-30T00:43:35.416166Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = TextEmbedding(model_name=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Comparison\n",
    "\n",
    "We'll be comparing the following metrics: Minimum, Maximum, Mean, across k runs. Let's write a function to do that:\n",
    "\n",
    "### ðŸš€ Calculating Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:43:35.693539Z",
     "start_time": "2024-03-30T00:43:35.488973Z"
    }
   },
   "outputs": [],
   "source": [
    "import types\n",
    "\n",
    "\n",
    "def calculate_time_stats(embed_func: Callable, documents: list, k: int) -> Tuple[float, float, float]:\n",
    "    times = []\n",
    "    for _ in range(k):\n",
    "        # Timing the embed_func call\n",
    "        start_time = time.time()\n",
    "        embeddings = embed_func(documents)\n",
    "        # Force computation if embed_func returns a generator\n",
    "        if isinstance(embeddings, types.GeneratorType):\n",
    "            list(embeddings)\n",
    "\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "\n",
    "    # Returning mean, max, and min time for the call\n",
    "    return (sum(times) / k, max(times), min(times))\n",
    "\n",
    "\n",
    "hf_stats = calculate_time_stats(hf.embed, documents, k=2)\n",
    "print(f\"Huggingface Transformers (Average, Max, Min): {hf_stats}\")\n",
    "fst_stats = calculate_time_stats(embedding_model.embed, documents, k=2)\n",
    "print(f\"FastEmbed (Average, Max, Min): {fst_stats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Results\n",
    "\n",
    "Let's run the comparison and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:43:35.746781Z",
     "start_time": "2024-03-30T00:43:35.698423Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_character_per_second_comparison(\n",
    "    hf_stats: Tuple[float, float, float], fst_stats: Tuple[float, float, float], documents: list\n",
    "):\n",
    "    # Calculating total characters in documents\n",
    "    total_characters = sum(len(doc) for doc in documents)\n",
    "\n",
    "    # Calculating characters per second for each model\n",
    "    hf_chars_per_sec = total_characters / hf_stats[0]  # Mean time is at index 0\n",
    "    fst_chars_per_sec = total_characters / fst_stats[0]\n",
    "\n",
    "    # Plotting the bar chart\n",
    "    models = [\"HF Embed (Torch)\", \"FastEmbed\"]\n",
    "    chars_per_sec = [hf_chars_per_sec, fst_chars_per_sec]\n",
    "\n",
    "    bars = plt.bar(models, chars_per_sec, color=[\"#1f356c\", \"#dd1f4b\"])\n",
    "    plt.ylabel(\"Characters per Second\")\n",
    "    plt.title(\"Characters Processed per Second Comparison\")\n",
    "\n",
    "    # Adding the number at the top of each bar\n",
    "    for bar, chars in zip(bars, chars_per_sec):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            bar.get_height(),\n",
    "            f\"{chars:.1f}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            color=\"#1f356c\",\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_character_per_second_comparison(hf_stats, fst_stats, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are the Embeddings the same?\n",
    "\n",
    "This is a very important question. Let's see if the embeddings are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:43:25.537072Z",
     "start_time": "2024-03-30T00:43:25.419184Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(embeddings1: Tensor, embeddings2: Tensor) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two sets of embeddings\n",
    "    \"\"\"\n",
    "    return F.cosine_similarity(embeddings1, embeddings2).mean().item()\n",
    "\n",
    "\n",
    "calculate_cosine_similarity(hf.embed(documents), Tensor(list(embedding_model.embed(documents))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates the embeddings are quite close to each with a cosine similarity of 0.99 for BAAI/bge-small-en and 0.92 for BAAI/bge-small-en-v1.5. This gives us confidence that the embeddings are the same and we are not sacrificing accuracy for speed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
