{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\u26a1\ufe0f What is FastEmbed?","text":"<p>FastEmbed is a lightweight, fast, Python library built for embedding generation. We support popular text models. Please open a Github issue if you want us to add a new model.</p> <ol> <li> <p>Light &amp; Fast</p> <ul> <li>Quantized model weights</li> <li>ONNX Runtime for inference</li> </ul> </li> <li> <p>Accuracy/Recall</p> <ul> <li>Better than OpenAI Ada-002</li> <li>Default is Flag Embedding, which has shown good results on the MTEB leaderboard</li> <li>List of supported models - including multilingual models</li> </ul> </li> </ol> <p>Here is an example for Retrieval Embedding Generation and how to use FastEmbed with Qdrant.</p>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<p>To install the FastEmbed library, pip works:</p> <pre><code>pip install fastembed\n</code></pre>"},{"location":"#usage","title":"\ud83d\udcd6 Usage","text":"<pre><code>from fastembed import TextEmbedding\n\ndocuments: list[str] = [\n    \"passage: Hello, World!\",\n    \"query: Hello, World!\",\n    \"passage: This is an example passage.\",\n    \"fastembed is supported by and maintained by Qdrant.\"\n]\nembedding_model = TextEmbedding()\nembeddings: list[np.ndarray] = embedding_model.embed(documents)\n</code></pre>"},{"location":"#usage-with-qdrant","title":"Usage with Qdrant","text":"<p>Installation with Qdrant Client in Python:</p> <pre><code>pip install qdrant-client[fastembed]\n</code></pre> <p>Might have to use <code>pip install 'qdrant-client[fastembed]'</code> on zsh.</p> <pre><code>from qdrant_client import QdrantClient\n\n# Initialize the client\nclient = QdrantClient(\":memory:\")  # Using an in-process Qdrant\n\n# Prepare your documents, metadata, and IDs\ndocs = [\"Qdrant has Langchain integrations\", \"Qdrant also has Llama Index integrations\"]\nmetadata = [\n    {\"source\": \"Langchain-docs\"},\n    {\"source\": \"Llama-index-docs\"},\n]\nids = [42, 2]\n\nclient.add(\n    collection_name=\"demo_collection\",\n    documents=docs,\n    metadata=metadata,\n    ids=ids\n)\n\nsearch_result = client.query(\n    collection_name=\"demo_collection\",\n    query_text=\"This is a query document\"\n)\nprint(search_result)\n</code></pre>"},{"location":"Getting%20Started/","title":"Getting Started","text":"<pre><code>!pip install -Uqq fastembed\n</code></pre> <pre><code>import numpy as np\n\nfrom fastembed import TextEmbedding\n\n\n# Example list of documents\ndocuments: list[str] = [\n    \"This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\",\n    \"fastembed is supported by and maintained by Qdrant.\",\n]\n\n# This will trigger the model download and initialization\nembedding_model = TextEmbedding()\nprint(\"The model BAAI/bge-small-en-v1.5 is ready to use.\")\n\nembeddings_generator = embedding_model.embed(documents)\nembeddings_list = list(embeddings_generator)\nlen(embeddings_list[0])  # Vector of 384 dimensions\n</code></pre> <pre>\n<code>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>The model BAAI/bge-small-en-v1.5 is ready to use.\n</code>\n</pre> <pre>\n<code>384</code>\n</pre> <p>&gt; \ud83d\udca1 Why do we use generators? &gt;  &gt; We use them to save memory mostly. Instead of loading all the vectors into memory, we can load them one by one. This is useful when you have a large dataset and you don't want to load all the vectors at once.</p> <pre><code>embeddings_generator = embedding_model.embed(documents)\n\nfor doc, vector in zip(documents, embeddings_generator):\n    print(\"Document:\", doc)\n    print(f\"Vector of type: {type(vector)} with shape: {vector.shape}\")\n</code></pre> <pre>\n<code>Document: This is built to be faster and lighter than other embedding libraries e.g. Transformers, Sentence-Transformers, etc.\nVector of type: &lt;class 'numpy.ndarray'&gt; with shape: (384,)\nDocument: fastembed is supported by and maintained by Qdrant.\nVector of type: &lt;class 'numpy.ndarray'&gt; with shape: (384,)\n</code>\n</pre> <pre><code>embeddings_list = np.array(list(embedding_model.embed(documents)))\nembeddings_list.shape\n</code></pre> <pre>\n<code>(2, 384)</code>\n</pre> <p>We're using BAAI/bge-small-en-v1.5 a state of the art Flag Embedding model. The model does better than OpenAI text-embedding-ada-002. We've made it even faster by converting it to ONNX format and quantizing the model for you.</p> <pre><code>multilingual_large_model = TextEmbedding(\"intfloat/multilingual-e5-large\")\n</code></pre> <pre>\n<code>Fetching 8 files:   0%|          | 0/8 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>np.array(\n    list(multilingual_large_model.embed([\"Hello, world!\", \"\u4f60\u597d\u4e16\u754c\", \"\u00a1Hola Mundo!\", \"\u0928\u092e\u0938\u094d\u0924\u0947!\"]))\n).shape  # Vector of 1024 dimensions\n</code></pre> <pre>\n<code>(4, 1024)</code>\n</pre> <p>Next: Checkout how to use FastEmbed with Qdrant for similarity search: FastEmbed with Qdrant</p>"},{"location":"Getting%20Started/#getting-started","title":"\ud83d\udeb6\ud83c\udffb\u200d\u2642\ufe0f Getting Started","text":"<p>Here you will learn how to use the fastembed package to embed your data into a vector space. The package is designed to be easy to use and fast. It is built on top of the ONNX standard, which allows for fast inference on a variety of hardware (called Runtimes in ONNX). </p>"},{"location":"Getting%20Started/#quick-start","title":"Quick Start","text":"<p>The fastembed package is designed to be easy to use. We'll be using <code>TextEmbedding</code> class. It takes a list of strings as input and returns a generator of vectors.</p> <p>&gt; \ud83d\udca1 You can learn more about generators from Python Wiki</p>"},{"location":"Getting%20Started/#format-of-the-document-list","title":"Format of the Document List","text":"<ol> <li>List of Strings: Your documents must be in a list, and each document must be a string</li> <li>For Retrieval Tasks with our default: If you're working with queries and passages, you can add special labels to them:</li> <li>Queries: Add \"query:\" at the beginning of each query string</li> <li>Passages: Add \"passage:\" at the beginning of each passage string</li> </ol>"},{"location":"Getting%20Started/#beyond-the-default-model","title":"Beyond the default model","text":"<p>The default model is built for speed and efficiency. If you need a more accurate model, you can use the <code>TextEmbedding</code> class to load any model from our list of available models. You can find the list of available models using <code>TextEmbedding.list_supported_models()</code>.</p>"},{"location":"examples/ColBERT_with_FastEmbed/","title":"ColBERT with FastEmbed","text":"<pre><code>from fastembed import LateInteractionTextEmbedding\n\nLateInteractionTextEmbedding.list_supported_models()\n</code></pre> <pre>\n<code>/Users/joein/work/qdrant/fastembed/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</code>\n</pre> <pre>\n<code>[{'model': 'colbert-ir/colbertv2.0',\n  'dim': 128,\n  'description': 'Late interaction model',\n  'size_in_GB': 0.44,\n  'sources': {'hf': 'colbert-ir/colbertv2.0'},\n  'model_file': 'model.onnx'}]</code>\n</pre> <pre><code>embedding_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")\n</code></pre> <pre>\n<code>Fetching 5 files:   0%|          | 0/5 [00:00&lt;?, ?it/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 743/743 [00:00&lt;00:00, 4.56MB/s]\n\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 405/405 [00:00&lt;00:00, 3.34MB/s]\nFetching 5 files:  20%|\u2588\u2588        | 1/5 [00:00&lt;00:01,  3.64it/s]\ntokenizer.json:   0%|          | 0.00/466k [00:00&lt;?, ?B/s]\n\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:00&lt;00:00, 727kB/s]\n\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 466k/466k [00:00&lt;00:00, 1.48MB/s]\n\nmodel.onnx:   0%|          | 0.00/436M [00:00&lt;?, ?B/s]\nmodel.onnx:   2%|\u258f         | 10.5M/436M [00:00&lt;00:34, 12.2MB/s]\nmodel.onnx:   5%|\u258d         | 21.0M/436M [00:01&lt;00:20, 20.3MB/s]\nmodel.onnx:   7%|\u258b         | 31.5M/436M [00:01&lt;00:15, 25.7MB/s]\nmodel.onnx:  10%|\u2589         | 41.9M/436M [00:01&lt;00:13, 29.4MB/s]\nmodel.onnx:  12%|\u2588\u258f        | 52.4M/436M [00:01&lt;00:12, 31.9MB/s]\nmodel.onnx:  14%|\u2588\u258d        | 62.9M/436M [00:02&lt;00:11, 33.7MB/s]\nmodel.onnx:  17%|\u2588\u258b        | 73.4M/436M [00:02&lt;00:10, 34.9MB/s]\nmodel.onnx:  19%|\u2588\u2589        | 83.9M/436M [00:02&lt;00:09, 35.5MB/s]\nmodel.onnx:  22%|\u2588\u2588\u258f       | 94.4M/436M [00:03&lt;00:09, 36.1MB/s]\nmodel.onnx:  24%|\u2588\u2588\u258d       | 105M/436M [00:03&lt;00:09, 36.6MB/s] \nmodel.onnx:  26%|\u2588\u2588\u258b       | 115M/436M [00:03&lt;00:08, 36.9MB/s]\nmodel.onnx:  29%|\u2588\u2588\u2589       | 126M/436M [00:03&lt;00:08, 37.1MB/s]\nmodel.onnx:  31%|\u2588\u2588\u2588\u258f      | 136M/436M [00:04&lt;00:08, 37.3MB/s]\nmodel.onnx:  34%|\u2588\u2588\u2588\u258e      | 147M/436M [00:04&lt;00:07, 37.4MB/s]\nmodel.onnx:  36%|\u2588\u2588\u2588\u258c      | 157M/436M [00:04&lt;00:07, 37.4MB/s]\nmodel.onnx:  38%|\u2588\u2588\u2588\u258a      | 168M/436M [00:05&lt;00:07, 37.5MB/s]\nmodel.onnx:  41%|\u2588\u2588\u2588\u2588      | 178M/436M [00:05&lt;00:06, 37.6MB/s]\nmodel.onnx:  43%|\u2588\u2588\u2588\u2588\u258e     | 189M/436M [00:05&lt;00:06, 37.6MB/s]\nmodel.onnx:  46%|\u2588\u2588\u2588\u2588\u258c     | 199M/436M [00:05&lt;00:06, 37.6MB/s]\nmodel.onnx:  48%|\u2588\u2588\u2588\u2588\u258a     | 210M/436M [00:06&lt;00:06, 37.5MB/s]\nmodel.onnx:  50%|\u2588\u2588\u2588\u2588\u2588     | 220M/436M [00:06&lt;00:05, 37.5MB/s]\nmodel.onnx:  53%|\u2588\u2588\u2588\u2588\u2588\u258e    | 231M/436M [00:06&lt;00:05, 37.6MB/s]\nmodel.onnx:  55%|\u2588\u2588\u2588\u2588\u2588\u258c    | 241M/436M [00:06&lt;00:05, 37.6MB/s]\nmodel.onnx:  58%|\u2588\u2588\u2588\u2588\u2588\u258a    | 252M/436M [00:07&lt;00:04, 37.6MB/s]\nmodel.onnx:  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 262M/436M [00:07&lt;00:04, 37.7MB/s]\nmodel.onnx:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u258e   | 273M/436M [00:07&lt;00:04, 37.7MB/s]\nmodel.onnx:  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u258d   | 283M/436M [00:08&lt;00:04, 36.0MB/s]\nmodel.onnx:  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u258b   | 294M/436M [00:08&lt;00:03, 36.4MB/s]\nmodel.onnx:  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2589   | 304M/436M [00:08&lt;00:03, 36.8MB/s]\nmodel.onnx:  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 315M/436M [00:08&lt;00:03, 37.0MB/s]\nmodel.onnx:  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d  | 325M/436M [00:09&lt;00:02, 37.3MB/s]\nmodel.onnx:  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 336M/436M [00:09&lt;00:03, 30.8MB/s]\nmodel.onnx:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 346M/436M [00:10&lt;00:02, 32.6MB/s]\nmodel.onnx:  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f | 357M/436M [00:10&lt;00:02, 33.9MB/s]\nmodel.onnx:  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d | 367M/436M [00:10&lt;00:01, 34.8MB/s]\nmodel.onnx:  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b | 377M/436M [00:10&lt;00:01, 35.7MB/s]\nmodel.onnx:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 388M/436M [00:11&lt;00:01, 36.2MB/s]\nmodel.onnx:  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f| 398M/436M [00:11&lt;00:01, 36.6MB/s]\nmodel.onnx:  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 409M/436M [00:11&lt;00:00, 36.9MB/s]\nmodel.onnx:  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c| 419M/436M [00:11&lt;00:00, 37.1MB/s]\nmodel.onnx:  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a| 430M/436M [00:12&lt;00:00, 37.3MB/s]\nmodel.onnx: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 436M/436M [00:12&lt;00:00, 35.1MB/s]\nFetching 5 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:13&lt;00:00,  2.68s/it]\n</code>\n</pre> <pre><code>documents = [\n    \"ColBERT is a late interaction text embedding model, however, there are also other models such as TwinBERT.\",\n    \"On the contrary to the late interaction models, the early interaction models contains interaction steps at embedding generation process\",\n]\nqueries = [\n    \"Are there any other late interaction text embedding models except ColBERT?\",\n    \"What is the difference between late interaction and early interaction text embedding models?\",\n]\n</code></pre> <p>NOTE: ColBERT computes query and documents embeddings differently, make sure to use the corresponding methods.</p> <pre><code>document_embeddings = list(\n    embedding_model.embed(documents)\n)  # embed and qury_embed return generators,\n# which we need to evaluate by writing them to a list\nquery_embeddings = list(embedding_model.query_embed(queries))\n</code></pre> <pre><code>document_embeddings[0].shape, query_embeddings[0].shape\n</code></pre> <pre>\n<code>((26, 128), (32, 128))</code>\n</pre> <p>Don't worry about query embeddings having the bigger shape in this case.  ColBERT authors recommend to pad queries with [MASK] tokens to 32 tokens. They also recommends to truncate queries to 32 tokens, however we don't do that in FastEmbed, so you can put some straight into the queries.</p> <p>Qdrant will support ColBERT as of the next version (v1.10), however, at the moment, you can compute embedding similarities manually.   </p> <pre><code>import numpy as np\n\n\ndef compute_relevance_scores(\n    query_embedding: np.array, document_embeddings: np.array, k: int\n) -&amp;gt; list[int]:\n    \"\"\"\n    Compute relevance scores for top-k documents given a query.\n\n    :param query_embedding: Numpy array representing the query embedding, shape: [num_query_terms, embedding_dim]\n    :param document_embeddings: Numpy array representing embeddings for documents, shape: [num_documents, max_doc_length, embedding_dim]\n    :param k: Number of top documents to return\n    :return: Indices of the top-k documents based on their relevance scores\n    \"\"\"\n    # Compute batch dot-product of query_embedding and document_embeddings\n    # Resulting shape: [num_documents, num_query_terms, max_doc_length]\n    scores = np.matmul(query_embedding, document_embeddings.transpose(0, 2, 1))\n\n    # Apply max-pooling across document terms (axis=2) to find the max similarity per query term\n    # Shape after max-pool: [num_documents, num_query_terms]\n    max_scores_per_query_term = np.max(scores, axis=2)\n\n    # Sum the scores across query terms to get the total score for each document\n    # Shape after sum: [num_documents]\n    total_scores = np.sum(max_scores_per_query_term, axis=1)\n\n    # Sort the documents based on their total scores and get the indices of the top-k documents\n    sorted_indices = np.argsort(total_scores)[::-1][:k]\n\n    return sorted_indices\n</code></pre> <pre><code>sorted_indices = compute_relevance_scores(\n    np.array(query_embeddings[0]), np.array(document_embeddings), k=3\n)\nprint(\"Sorted document indices:\", sorted_indices)\n</code></pre> <pre>\n<code>Sorted document indices: [0 1]\n</code>\n</pre> <pre><code>print(f\"Query: {queries[0]}\")\nfor index in sorted_indices:\n    print(f\"Document: {documents[index]}\")\n</code></pre> <pre>\n<code>Query: Are there any other late interaction text embedding models except ColBERT?\nDocument: ColBERT is a late interaction text embedding model, however, there are also other models such as TwinBERT.\nDocument: On the contrary to the late interaction models, the early interaction models contains interaction steps at embedding generation process\n</code>\n</pre> <p>Despite ColBERT allows to compute embeddings independently and spare some workload offline, it still computes more resources than no interaction models. Due to this, it might be more reasonable to use ColBERT not as a first-stage retriever, but as a re-ranker.</p> <p>The first-stage retriever would then be a no-interaction model, which e.g. retrieves first 100 or 500 examples, and leave the final ranking to the ColBERT model.</p> <pre><code>\n</code></pre>"},{"location":"examples/ColBERT_with_FastEmbed/#late-interaction-text-embedding-models","title":"Late Interaction Text Embedding Models","text":"<p>As of version 0.3.0 FastEmbed supports Late Interaction Text Embedding Models and currently available with one of the most popular embedding model of the family - ColBERT.</p>"},{"location":"examples/ColBERT_with_FastEmbed/#what-is-a-late-interaction-text-embedding-model","title":"What is a Late Interaction Text Embedding Model?","text":"<p>Late Interaction Text Embedding Model is a kind of information retrieval model which performs query and documents interactions at the scoring stage. In order to better understand it, we can compare it to the models without interaction. For instance, if you take a sentence-transformer model, compute embeddings for your documents, compute embeddings for your queries, and just compare them by cosine similarity, then you're retrieving points without interaction.</p> <p>It is a pretty much easy and straightforward approach, however we might be sacrificing some precision due to its simplicity. It is caused by several facts:  - there is no interaction between queries and documents at the early stage (embedding generation) nor at the late stage (during scoring).  - we are trying to encapsulate all the document information in only one pooled embedding, and obviously, some information might be lost.</p> <p>Late Interaction Text Embedding models are trying to address it by computing embeddings for each token in queries and documents, and then finding the most similar ones via model specific operation, e.g. ColBERT (Contextual Late Interaction over BERT) uses MaxSim operation. With this approach we can have not only a better representation of the documents, but also make queries and documents more aware one of another.</p> <p>For more information on ColBERT and MaxSim operation, you can check out this blogpost by Jina AI.</p>"},{"location":"examples/ColBERT_with_FastEmbed/#colbert-in-fastembed","title":"ColBERT in FastEmbed","text":"<p>FastEmbed provides a simple way to use ColBERT model, similar to the ones it has with <code>TextEmbedding</code>.</p>"},{"location":"examples/ColBERT_with_FastEmbed/#maxsim-operator","title":"MaxSim operator","text":""},{"location":"examples/ColBERT_with_FastEmbed/#use-case-recommendation","title":"Use-case recommendation","text":""},{"location":"examples/FastEmbed_GPU/","title":"FastEmbed GPU","text":"<pre><code>!pip install fastembed-gpu\n</code></pre> <p>NOTE: <code>onnxruntime-gpu</code> and <code>onnxruntime</code> can't be installed in the same environment. If you have <code>onnxruntime</code> installed, you would need to uninstall it before installing <code>onnxruntime-gpu</code>. Same is true for <code>fastembed</code> and <code>fastembed-gpu</code>.</p> <pre><code>!sudo apt install cudnn9\n!pip install fastembed-gpu -qqq\n</code></pre> <p>If it necessary to work with CuDNN 8, you can consider locking <code>onnxruntime-gpu</code> to 1.18.0 with CUDA 12.x by this command:</p> <pre><code>!pip install onnxruntime-gpu==1.18.0 -i https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/ -qq\n!pip install fastembed-gpu -qqq\n</code></pre> <pre><code>!pip install onnxruntime-gpu -i https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-11/pypi/simple/ -qq\n</code></pre> <p>NOTE: Ensure that CuDNN 9.x is installed when working with the latest <code>onnxruntime-gpu</code>, whether using CUDA 11.x or 12.x.</p> <pre><code>import numpy as np\n\nfrom fastembed import TextEmbedding\n\nembedding_model_gpu = TextEmbedding(\n    model_name=\"BAAI/bge-small-en-v1.5\", providers=[\"CUDAExecutionProvider\"]\n)\nembedding_model_gpu.model.model.get_providers()\n</code></pre> <pre>\n<code>/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \nThe secret `HF_TOKEN` does not exist in your Colab secrets.\nTo authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\nYou will be able to reuse this secret in all of your notebooks.\nPlease note that authentication is recommended but still optional to access public models or datasets.\n  warnings.warn(\n</code>\n</pre> <pre>\n<code>Fetching 5 files:   0%|          | 0/5 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>tokenizer_config.json:   0%|          | 0.00/1.24k [00:00&lt;?, ?B/s]</code>\n</pre> <pre>\n<code>config.json:   0%|          | 0.00/706 [00:00&lt;?, ?B/s]</code>\n</pre> <pre>\n<code>special_tokens_map.json:   0%|          | 0.00/695 [00:00&lt;?, ?B/s]</code>\n</pre> <pre>\n<code>tokenizer.json:   0%|          | 0.00/711k [00:00&lt;?, ?B/s]</code>\n</pre> <pre>\n<code>model_optimized.onnx:   0%|          | 0.00/66.5M [00:00&lt;?, ?B/s]</code>\n</pre> <pre>\n<code>['CUDAExecutionProvider', 'CPUExecutionProvider']</code>\n</pre> <pre><code>documents: list[str] = list(np.repeat(\"Demonstrating GPU acceleration in fastembed\", 500))\n</code></pre> <pre><code>%%timeit\nlist(embedding_model_gpu.embed(documents))\n</code></pre> <pre>\n<code>43.4 ms \u00b1 2.06 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code>\n</pre> <pre><code>embedding_model_cpu = TextEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\nembedding_model_cpu.model.model.get_providers()\n</code></pre> <pre>\n<code>Fetching 5 files:   0%|          | 0/5 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>['CPUExecutionProvider']</code>\n</pre> <pre><code>%%timeit\nlist(embedding_model_cpu.embed(documents))\n</code></pre> <pre>\n<code>4.33 s \u00b1 591 ms per loop (mean \u00b1 std. dev. of 7 runs, 1 loop each)\n</code>\n</pre>"},{"location":"examples/FastEmbed_GPU/#fastembed-on-gpu","title":"FastEmbed on GPU","text":"<p>As of version 0.2.7 FastEmbed supports GPU acceleration.</p> <p>This notebook covers the installation process and usage of fastembed on GPU.</p>"},{"location":"examples/FastEmbed_GPU/#installation","title":"Installation","text":"<p>Fastembed depends on <code>onnxruntime</code> and inherits its scheme of GPU support.</p> <p>In order to use GPU with onnx models, you would need to have <code>onnxruntime-gpu</code> package, which substitutes all the <code>onnxruntime</code> functionality. Fastembed mimics this behavior and requires <code>fastembed-gpu</code> package to be installed.</p>"},{"location":"examples/FastEmbed_GPU/#cuda-12x-support","title":"CUDA 12.x support","text":"<p>You can check your CUDA version using such commands as <code>nvidia-smi</code> or <code>nvcc --version</code></p> <p>Starting from version 1.19.0, onnxruntime-gpu ships with support for CUDA 12.x by default.</p> <p>Google Colab notebooks have by default CUDA 12.x and CuDNN 8.x.</p> <p>Latest version of <code>onnxruntime-gpu</code> requires CuDNN 9.x, in order to install it you can run the following command: </p>"},{"location":"examples/FastEmbed_GPU/#cuda-11x-support","title":"CUDA 11.x support","text":"<p>To use latest version of <code>onnxruntime-gpu</code> with CUDA 11.x, you can run the following command:</p>"},{"location":"examples/FastEmbed_GPU/#cuda-drivers","title":"CUDA drivers","text":"<p>FastEmbed does not include CUDA drivers and CuDNN libraries. You would need to take care of the environment setup on your own. The dependencies required for the chosen onnxruntime version are listed in the CUDA Execution Provider requirements.</p>"},{"location":"examples/FastEmbed_GPU/#setting-up-fastembed-gpu-on-gcp","title":"Setting up fastembed-gpu on GCP","text":""},{"location":"examples/FastEmbed_GPU/#cuda-drivers_1","title":"CUDA drivers","text":"<p>CUDA 11.8 toolkit or CUDA 12.x toolkit has to be installed if they haven't yet been set up.</p>"},{"location":"examples/FastEmbed_GPU/#example-of-setting-up-cuda-12x-on-ubuntu-2204","title":"Example of setting up CUDA 12.x on Ubuntu 22.04","text":"<p>Make sure to download an archive which has been created for your particular platform, CPU architecture and OS distribution.</p> <p>For Ubuntu 22.04 with x86_64 CPU architecture the following archive has to be downloaded.</p> <p><pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda\n</code></pre> NOTE: Specific CUDA libraries can be found in the meta packages section in the CUDA installation guide.</p> <p>NOTE: When installing CUDA, the environment variable might not be set by default. Make sure to add the following line to your environment variables: <pre><code>LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n</code></pre> This will ensure that the CUDA libraries are properly linked.</p>"},{"location":"examples/FastEmbed_GPU/#cudnn-9x","title":"CuDNN 9.x","text":"<p>CuDNN 9.x library can be installed via the following archive.</p>"},{"location":"examples/FastEmbed_GPU/#example-of-setting-up-cudnn-9x-on-ubuntu-2204","title":"Example of setting up CuDNN 9.x on Ubuntu 22.04","text":"<p>CuDNN 9.x for Ubuntu 22.04 x86_64 archive can be downloaded and installed in the following way: <pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cudnn\n</code></pre> NOTE: When installing CuDNN, you can choose specific version, cudnn-cuda-11 or cudnn-cuda-12</p>"},{"location":"examples/FastEmbed_GPU/#common-issues","title":"Common issues","text":"<p>The following are some common issues that may arise while using <code>fastembed-gpu</code> if not installed properly:</p> <p>CUDA library is not installed: <pre><code>FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcublasLt.so.x: cannot open shared object file: No such file or directory\n</code></pre></p> <p>CuDNN library is not installed: <pre><code>FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcudnn.so.x: cannot open shared object file: No such file or directory\n</code></pre></p> <p>CUDA library path is not set: <pre><code>FAIL : Failed to load library libonnxruntime_providers_cuda.so with error: libcufft.so.x: failed to map segment from shared object\n</code></pre></p> <p>Make sure to add the following line to your environment variables: <pre><code>LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\n</code></pre></p>"},{"location":"examples/FastEmbed_GPU/#usage","title":"Usage","text":""},{"location":"examples/FastEmbed_Multi_GPU/","title":"FastEmbed Multi GPU","text":"<pre><code>from fastembed import TextEmbedding\n\n# define the documents to embed\ndocs = [\"hello world\", \"flag embedding\"] * 100\n\n# define gpu ids\ndevice_ids = [0, 1]\n\nif __name__ == \"__main__\":\n    # initialize a TextEmbedding model using CUDA\n    text_model = TextEmbedding(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n        cuda=True,\n        device_ids=device_ids,\n        lazy_load=True,\n    )\n\n    # generate embeddings\n    text_embeddings = list(text_model.embed(docs, batch_size=2, parallel=len(device_ids)))\n    print(text_embeddings)\n</code></pre> <p>In this snippet: - <code>cuda=True</code> enables GPU acceleration. - <code>device_ids=[0, 1]</code> specifies GPUs to use. Replace <code>[0, 1]</code> with available GPU IDs. - <code>lazy_load=True</code></p> <p>NOTE: When using multi-GPU settings, it is important to configure <code>parallel</code> and <code>lazy_load</code> properly to avoid inefficiencies:</p> <p><code>parallel</code>: This parameter enables multi-GPU support by spawning child processes for each GPU specified in device_ids. To ensure proper utilization, the value of <code>parallel</code> must match the number of GPUs in device_ids. If using a single GPU, this parameter is not necessary.</p> <p><code>lazy_load</code>: Enabling <code>lazy_load</code> prevents redundant memory usage. Without <code>lazy_load</code>, the model is initially loaded into the memory of the first GPU by the main process. When child processes are spawned for each GPU, the model is reloaded on the first GPU, causing redundant memory consumption and inefficiencies.</p>"},{"location":"examples/FastEmbed_Multi_GPU/#fastembed-multi-gpu-tutorial","title":"Fastembed Multi-GPU Tutorial","text":"<p>This tutorial demonstrates how to leverage multi-GPU support in Fastembed. Fastembed supports embedding text and images utilizing modern GPUs for acceleration. Let's explore how to use Fastembed with multiple GPUs step by step.</p>"},{"location":"examples/FastEmbed_Multi_GPU/#prerequisites","title":"Prerequisites","text":"<p>To get started, ensure you have the following installed: - Python 3.9 or later - Fastembed (<code>pip install fastembed-gpu</code>) - Refer to this tutorial if you have issues with GPU dependencies - Access to a multi-GPU server</p>"},{"location":"examples/FastEmbed_Multi_GPU/#multi-gpu-using-cuda-argument-with-textembedding-model","title":"Multi-GPU using cuda argument with TextEmbedding Model","text":""},{"location":"examples/FastEmbed_vs_HF_Comparison/","title":"FastEmbed vs HF Comparison","text":"<pre><code>!pip install matplotlib transformers torch -qq\n</code></pre> <pre><code>import time\nfrom typing import Callable\n\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom transformers import AutoModel, AutoTokenizer\n\nfrom fastembed import TextEmbedding\n</code></pre> <pre><code>import fastembed\n\nfastembed.__version__\n</code></pre> <pre>\n<code>'0.2.6'</code>\n</pre> <pre><code>documents: list[str] = [\n    \"Chandrayaan-3 is India's third lunar mission\",\n    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n    \"The estimated cost of the mission is around $35 million\",\n    \"It will carry instruments to study the lunar surface and atmosphere\",\n    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n    \"The mission used GSLV Mk III rocket for its launch\",\n    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n    \"Chandrayaan-3 was launched earlier in the year 2023\",\n]\nlen(documents)\n</code></pre> <pre>\n<code>12</code>\n</pre> <pre><code>class HF:\n    \"\"\"\n    HuggingFace Transformer implementation of FlagEmbedding\n    \"\"\"\n\n    def __init__(self, model_id: str) -&amp;gt; None:\n        self.model = AutoModel.from_pretrained(model_id)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n    def embed(self, texts: list[str]):\n        encoded_input = self.tokenizer(\n            texts, max_length=512, padding=True, truncation=True, return_tensors=\"pt\"\n        )\n        model_output = self.model(**encoded_input)\n        sentence_embeddings = model_output[0][:, 0]\n        sentence_embeddings = F.normalize(sentence_embeddings)\n        return sentence_embeddings\n\n\nmodel_id = \"BAAI/bge-small-en-v1.5\"\nhf = HF(model_id=model_id)\nhf.embed(documents).shape\n</code></pre> <pre>\n<code>torch.Size([12, 384])</code>\n</pre> <pre><code>embedding_model = TextEmbedding(model_name=model_id)\n</code></pre> <pre>\n<code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</code>\n</pre> <pre>\n<code>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>import types\n\n\ndef calculate_time_stats(\n    embed_func: Callable, documents: list, k: int\n) -&amp;gt; tuple[float, float, float]:\n    times = []\n    for _ in range(k):\n        # Timing the embed_func call\n        start_time = time.time()\n        embeddings = embed_func(documents)\n        # Force computation if embed_func returns a generator\n        if isinstance(embeddings, types.GeneratorType):\n            list(embeddings)\n\n        end_time = time.time()\n        times.append(end_time - start_time)\n\n    # Returning mean, max, and min time for the call\n    return (sum(times) / k, max(times), min(times))\n\n\nhf_stats = calculate_time_stats(hf.embed, documents, k=100)\nprint(f\"Huggingface Transformers (Average, Max, Min): {hf_stats}\")\nfst_stats = calculate_time_stats(embedding_model.embed, documents, k=100)\nprint(f\"FastEmbed (Average, Max, Min): {fst_stats}\")\n</code></pre> <pre>\n<code>Huggingface Transformers (Average, Max, Min): (0.04711266994476318, 0.0658111572265625, 0.043084144592285156)\nFastEmbed (Average, Max, Min): (0.04384247303009033, 0.05654191970825195, 0.04293417930603027)\n</code>\n</pre> <pre><code>def plot_character_per_second_comparison(\n    hf_stats: tuple[float, float, float], fst_stats: tuple[float, float, float], documents: list\n):\n    # Calculating total characters in documents\n    total_characters = sum(len(doc) for doc in documents)\n\n    # Calculating characters per second for each model\n    hf_chars_per_sec = total_characters / hf_stats[0]  # Mean time is at index 0\n    fst_chars_per_sec = total_characters / fst_stats[0]\n\n    # Plotting the bar chart\n    models = [\"HF Embed (Torch)\", \"FastEmbed\"]\n    chars_per_sec = [hf_chars_per_sec, fst_chars_per_sec]\n\n    bars = plt.bar(models, chars_per_sec, color=[\"#1f356c\", \"#dd1f4b\"])\n    plt.ylabel(\"Characters per Second\")\n    plt.title(\"Characters Processed per Second Comparison\")\n\n    # Adding the number at the top of each bar\n    for bar, chars in zip(bars, chars_per_sec):\n        plt.text(\n            bar.get_x() + bar.get_width() / 2,\n            bar.get_height(),\n            f\"{chars:.1f}\",\n            ha=\"center\",\n            va=\"bottom\",\n            color=\"#1f356c\",\n            fontsize=12,\n        )\n\n    plt.show()\n\n\nplot_character_per_second_comparison(hf_stats, fst_stats, documents)\n</code></pre> <pre><code>def calculate_cosine_similarity(embeddings1: Tensor, embeddings2: Tensor) -&amp;gt; float:\n    \"\"\"\n    Calculate cosine similarity between two sets of embeddings\n    \"\"\"\n    return F.cosine_similarity(embeddings1, embeddings2).mean().item()\n\n\ncalculate_cosine_similarity(hf.embed(documents), Tensor(list(embedding_model.embed(documents))))\n</code></pre> <pre>\n<code>/var/folders/b4/grpbcmrd36gc7q5_11whbn540000gn/T/ipykernel_14307/1958479940.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n  calculate_cosine_similarity(hf.embed(documents), Tensor(list(embedding_model.embed(documents))))\n</code>\n</pre> <pre>\n<code>0.9999992847442627</code>\n</pre> <p>This indicates the embeddings are quite close to each with a cosine similarity of 0.99 for BAAI/bge-small-en and 0.92 for BAAI/bge-small-en-v1.5. This gives us confidence that the embeddings are the same and we are not sacrificing accuracy for speed.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#huggingface-vs-fastembed","title":"\ud83e\udd17 Huggingface vs \u26a1 FastEmbed\ufe0f","text":"<p>Comparing the performance of Huggingface's \ud83e\udd17 Transformers and \u26a1 FastEmbed\ufe0f on a simple task on the following machine: Apple M2 Max, 32 GB RAM.</p> <p></p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#imports","title":"\ud83d\udce6 Imports","text":"<p>Importing the necessary libraries for this comparison.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#data","title":"\ud83d\udcd6 Data","text":"<p>data is a list of strings, each string is a document.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#setting-up-huggingface","title":"Setting up \ud83e\udd17 Huggingface","text":"<p>We'll be using the Huggingface Transformers with PyTorch library to generate embeddings. We'll be using the same model across both libraries for a fair(er?) comparison.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#setting-up-fastembed","title":"Setting up \u26a1\ufe0fFastEmbed","text":"<p>Sorry, don't have a lot to set up here. We'll be using the default model, which is Flag Embedding, same as the Huggingface model.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#comparison","title":"\ud83d\udcca Comparison","text":"<p>We'll be comparing the following metrics: Minimum, Maximum, Mean, across k runs. Let's write a function to do that:</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#calculating-stats","title":"\ud83d\ude80 Calculating Stats","text":""},{"location":"examples/FastEmbed_vs_HF_Comparison/#results","title":"\ud83d\udcc8 Results","text":"<p>Let's run the comparison and see the results.</p>"},{"location":"examples/FastEmbed_vs_HF_Comparison/#are-the-embeddings-the-same","title":"Are the Embeddings the same?","text":"<p>This is a very important question. Let's see if the embeddings are the same.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/","title":"Hindi Tamil RAG with Navarasa7B","text":"Time: 25 min Level: Beginner Author Nirant Kasliwal <pre><code>!pip install -U fastembed datasets qdrant-client peft transformers accelerate bitsandbytes -qq\n</code></pre> <pre>\n<code>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n</code>\n</pre> <pre><code>import numpy as np\nfrom datasets import load_dataset\nfrom peft import AutoPeftModelForCausalLM\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import PointStruct, VectorParams, Distance\nfrom transformers import AutoTokenizer\n\nfrom fastembed import TextEmbedding\n</code></pre> <pre><code>hf_token = \"&lt;your_hf_token_here&gt;\"  # Get your token from https://huggingface.co/settings/token, needed for Gemma weights\n</code></pre> <pre><code>embedding_model = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\nmodel_id = \"Telugu-LLM-Labs/Indic-gemma-2b-finetuned-sft-Navarasa\"\n</code></pre> <pre><code>ds = load_dataset(\"nirantk/chaii-hindi-and-tamil-question-answering\", split=\"train\")\n</code></pre> <pre><code>ds\n</code></pre> <p>This dataset has questions and contexts which have corresponding answers. The answers must be found by the LLM. This is an extractive Question Answering problem.</p> <p>In order to do this, we'll setup an embedding model from FastEmbed. And then add it to Qdrant in memory mode, which is powered by Numpy.</p> <pre><code>embedding_model = TextEmbedding(model_name=embedding_model)\n</code></pre> <p>We'll use the 7B model here, the 2B model isn't great and was suffering from reading comprehension challenges.</p> <pre><code>model = AutoPeftModelForCausalLM.from_pretrained(\n    model_id,\n    load_in_4bit=False,\n    token=hf_token,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n</code></pre> <pre><code>questions, contexts = list(ds[\"question\"]), list(ds[\"context\"])\n</code></pre> <pre><code>context_embeddings: list[np.ndarray] = list(\n    embedding_model.embed(contexts)\n)  # Note the list() call - this is a generator\n</code></pre> <pre><code>len(context_embeddings[0])\n</code></pre> <pre><code>def embed_text(text: str) -&amp;gt; np.array:\n    return list(embedding_model.embed(text))[0]\n</code></pre> <pre><code>context_points = [\n    PointStruct(id=idx, vector=emb, payload={\"text\": text})\n    for idx, (emb, text) in enumerate(zip(context_embeddings, contexts))\n]\n</code></pre> <pre><code>len(context_points[0].vector)\n</code></pre> <pre><code>search_client = QdrantClient(\":memory:\")\n\nsearch_client.create_collection(\n    collection_name=\"hindi_tamil_contexts\",\n    vectors_config=VectorParams(size=len(context_points[0].vector), distance=Distance.COSINE),\n)\nsearch_client.upsert(collection_name=\"hindi_tamil_contexts\", points=context_points)\n</code></pre> <pre><code>idx = 997\n\nquestion = questions[idx]\nprint(question)\nsearch_context = search_client.search(\n    query_vector=embed_text(question), collection_name=\"hindi_tamil_contexts\", limit=2\n)\n</code></pre> <pre><code>search_context_text = search_context[0].payload[\"text\"]\nlen(search_context_text)\n</code></pre> <pre><code>input_prompt = \"\"\"\nAnswer the following question based on the context given after it in the same language as the question:\n### Question:\n{}\n\n### Context:\n{}\n\n### Answer:\n{}\"\"\"\n\ninput_text = input_prompt.format(\n    questions[idx],  # question\n    search_context_text[:2000],  # context\n    \"\",  # output - leave this blank for generation!\n)\n\ninputs = tokenizer([input_text], return_tensors=\"pt\")\n\noutputs = model.generate(**inputs, max_new_tokens=50, use_cache=True)\nresponse = tokenizer.batch_decode(outputs)[0]\n</code></pre> <pre><code>response.split(sep=\"### Answer:\")[-1].strip(\"&lt;eos&gt;\").strip()\n</code></pre> <pre><code>ds[idx][\"answer_text\"]\n</code></pre>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#hindi-and-tamil-question-answer-rag","title":"Hindi and Tamil Question Answer / RAG","text":"<p>In this notebook, we use new Navarasa LLMs from TeluguLLM to create a Hindi and Tamil Question Answering system. Since we're using a 7B model with PEFT, this notebook is run on Google Colab with an A100. If you're working with a smaller machine, I'd encourage to try the 2B model instead.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#setting-up","title":"Setting Up","text":"<p>We'll download the dataset, our LLM model weights and embedding model weights next</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#downloading-the-navarasa-llm","title":"Downloading the Navarasa LLM","text":"<p>We'll download the Navarasa LLM from TeluguLLM-Labs. This is a 7B model with PEFT.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#embed-the-context-into-vectors","title":"Embed the Context into Vectors","text":""},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#insert-into-qdrant","title":"Insert into Qdrant","text":""},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#selecting-a-question","title":"Selecting a Question","text":"<p>I've randomly selected a question here, with a specific and we then find the answer to it. We have the correct answer for it too -- so we can compare the two when you run the code.</p>"},{"location":"examples/Hindi_Tamil_RAG_with_Navarasa7B/#running-the-model-with-a-question-context","title":"Running the Model with a Question &amp; Context","text":""},{"location":"examples/Hybrid_Search/","title":"Hybrid Search","text":"<pre><code>!pip install -qU qdrant-client fastembed datasets transformers\n</code></pre> <pre><code>import json\n\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance,\n    NamedSparseVector,\n    NamedVector,\n    SparseVector,\n    PointStruct,\n    SearchRequest,\n    SparseIndexParams,\n    SparseVectorParams,\n    VectorParams,\n    ScoredPoint,\n)\nfrom transformers import AutoTokenizer\n\nimport fastembed\nfrom fastembed import SparseEmbedding, SparseTextEmbedding, TextEmbedding\n</code></pre> <pre><code>fastembed.__version__  # 0.2.5\n</code></pre> <pre>\n<code>'0.2.5'</code>\n</pre> <pre><code>dataset = load_dataset(\"tasksource/esci\", split=\"train\")\n# We'll select the first 1000 examples for this demo\ndataset = dataset.select(range(1000))\ndataset = dataset.filter(lambda x: x[\"product_locale\"] == \"us\")\ndataset\n</code></pre> <pre>\n<code>Dataset({\n    features: ['example_id', 'query', 'query_id', 'product_id', 'product_locale', 'esci_label', 'small_version', 'large_version', 'product_title', 'product_description', 'product_bullet_point', 'product_brand', 'product_color', 'product_text'],\n    num_rows: 919\n})</code>\n</pre> <pre><code>source_df = dataset.to_pandas()\ndf = source_df.drop_duplicates(\n    subset=[\"product_text\", \"product_title\", \"product_bullet_point\", \"product_brand\"]\n)\ndf = df.dropna(subset=[\"product_text\", \"product_title\", \"product_bullet_point\", \"product_brand\"])\ndf.head()\n</code></pre> example_id query query_id product_id product_locale esci_label small_version large_version product_title product_description product_bullet_point product_brand product_color product_text 0 0 revent 80 cfm 0 B000MOO21W us Irrelevant 0 1 Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceil... None WhisperCeiling fans feature a totally enclosed... Panasonic White Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceil... 2 1 revent 80 cfm 0 B07X3Y6B1V us Exact 0 1 Homewerks 7141-80 Bathroom Fan Integrated LED ... None OUTSTANDING PERFORMANCE: This Homewerk's bath ... Homewerks 80 CFM Homewerks 7141-80 Bathroom Fan Integrated LED ... 3 2 revent 80 cfm 0 B07WDM7MQQ us Exact 0 1 Homewerks 7140-80 Bathroom Fan Ceiling Mount E... None OUTSTANDING PERFORMANCE: This Homewerk's bath ... Homewerks White Homewerks 7140-80 Bathroom Fan Ceiling Mount E... 4 3 revent 80 cfm 0 B07RH6Z8KW us Exact 0 1 Delta Electronics RAD80L BreezRadiance 80 CFM ... This pre-owned or refurbished product has been... Quiet operation at 1.5 sones\\nBuilt-in thermos... DELTA ELECTRONICS (AMERICAS) LTD. White Delta Electronics RAD80L BreezRadiance 80 CFM ... 5 4 revent 80 cfm 0 B07QJ7WYFQ us Exact 0 1 Panasonic FV-08VRE2 Ventilation Fan with Reces... None The design solution for Fan/light combinations... Panasonic White Panasonic FV-08VRE2 Ventilation Fan with Reces... <pre><code>print(f\"Catalog Item Count: {len(df)}\\nQueries: {len(source_df)}\")\n</code></pre> <pre>\n<code>Catalog Item Count: 176\nQueries: 919\n</code>\n</pre> <pre><code>df[\"combined_text\"] = (\n    df[\"product_title\"] + \"\\n\" + df[\"product_text\"] + \"\\n\" + df[\"product_bullet_point\"]\n)\n</code></pre> <pre><code>len(df)\n</code></pre> <pre>\n<code>176</code>\n</pre> <pre><code>sparse_model_name = \"prithvida/Splade_PP_en_v1\"\ndense_model_name = \"BAAI/bge-large-en-v1.5\"\n# This triggers the model download\nsparse_model = SparseTextEmbedding(model_name=sparse_model_name, batch_size=32)\ndense_model = TextEmbedding(model_name=dense_model_name, batch_size=32)\n</code></pre> <pre>\n<code>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</code>\n</pre> <pre>\n<code>Fetching 7 files:   0%|          | 0/7 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>def make_sparse_embedding(texts: list[str]) -&amp;gt; list[SparseEmbedding]:\n    return list(sparse_model.embed(texts, batch_size=32))\n\n\nsparse_embedding: list[SparseEmbedding] = make_sparse_embedding(\n    [\"Fastembed is a great library for text embeddings!\"]\n)\nsparse_embedding\n</code></pre> <pre>\n<code>[SparseEmbedding(values=array([0.17295611, 0.80484658, 0.41356239, 0.38512513, 0.90825951,\n        0.61373132, 0.18313883, 0.18546289, 0.04257777, 1.20476401,\n        1.48403311, 0.17008089, 0.06487759, 0.16780744, 0.23214206,\n        2.5722568 , 1.87174428, 0.2541669 , 0.20749982, 0.16315481,\n        0.70712435, 0.26381177, 0.49152234, 0.67282563, 0.19267203,\n        0.29127747, 0.09682107, 1.21251154, 0.19741221, 0.44512141,\n        0.44369081, 0.21676107, 0.36704862, 0.06706504, 1.97674787,\n        0.00856015, 0.51626593, 0.21145488, 0.09790635, 0.26357391,\n        1.6925323 , 2.10766435, 0.05584541, 0.05150893, 0.24062614,\n        0.90479541, 0.1198509 , 0.10030396]), indices=array([ 1037,  2003,  2005,  2190,  2204,  2307,  2338,  2497,  2565,\n         2793,  3075,  3177,  3274,  3286,  3430,  3435,  3793,  3819,\n         3835,  3989,  4007,  4118,  4248,  4289,  4294,  4322,  4434,\n         4667,  4773,  5080,  5371,  5527,  6028,  6581,  6633,  6919,\n         6981,  6994,  7809,  7812,  7861,  8270,  9262,  9896, 10472,\n        13850, 16602, 23924]))]</code>\n</pre> <p>The previous output is a SparseEmbedding object for the first document in our list.</p> <p>It contains two arrays: values and indices.  - The 'values' array represents the weights of the features (tokens) in the document. - The 'indices' array represents the indices of these features in the model's vocabulary.</p> <p>Each pair of corresponding values and indices represents a token and its weight in the document.</p> <p>This is still a little abstract, so let's use the tokenizer vocab to make sense of these indices.</p> <pre><code>SparseTextEmbedding.list_supported_models()\n</code></pre> <pre>\n<code>[{'model': 'prithvida/Splade_PP_en_v1',\n  'vocab_size': 30522,\n  'description': 'Misspelled version of the model. Retained for backward compatibility. Independent Implementation of SPLADE++ Model for English',\n  'size_in_GB': 0.532,\n  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}},\n {'model': 'prithivida/Splade_PP_en_v1',\n  'vocab_size': 30522,\n  'description': 'Independent Implementation of SPLADE++ Model for English',\n  'size_in_GB': 0.532,\n  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}}]</code>\n</pre> <pre><code>def get_tokens_and_weights(sparse_embedding, model_name) -&amp;gt; dict[str, float]:\n    # Find the tokenizer for the model\n    tokenizer_source = None\n    for model_info in SparseTextEmbedding.list_supported_models():\n        if model_info[\"model\"].lower() == model_name.lower():\n            tokenizer_source = model_info[\"sources\"][\"hf\"]\n            break\n        else:\n            raise ValueError(f\"Model {model_name} not found in the supported models.\")\n\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_source)\n    token_weight_dict: dict[str, float] = {}\n    for i in range(len(sparse_embedding.indices)):\n        token = tokenizer.decode([sparse_embedding.indices[i]])\n        weight = sparse_embedding.values[i]\n        token_weight_dict[token] = weight\n\n    # Sort the dictionary by weights\n    token_weight_dict = dict(\n        sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)\n    )\n    return token_weight_dict\n\n\n# Test the function with the first SparseEmbedding\nprint(json.dumps(get_tokens_and_weights(sparse_embedding[0], sparse_model_name), indent=4))\n</code></pre> <pre>\n<code>{\n    \"fast\": 2.5722568035125732,\n    \"##bed\": 2.1076643466949463,\n    \"##em\": 1.9767478704452515,\n    \"text\": 1.8717442750930786,\n    \"em\": 1.6925323009490967,\n    \"library\": 1.4840331077575684,\n    \"##ding\": 1.2125115394592285,\n    \"bed\": 1.2047640085220337,\n    \"good\": 0.9082595109939575,\n    \"librarian\": 0.9047954082489014,\n    \"is\": 0.8048465847969055,\n    \"software\": 0.7071243524551392,\n    \"format\": 0.6728256344795227,\n    \"great\": 0.613731324672699,\n    \"texts\": 0.5162659287452698,\n    \"quick\": 0.49152234196662903,\n    \"device\": 0.4451214075088501,\n    \"file\": 0.44369080662727356,\n    \"for\": 0.4135623872280121,\n    \"best\": 0.38512513041496277,\n    \"technique\": 0.36704862117767334,\n    \"facility\": 0.2912774682044983,\n    \"method\": 0.26381176710128784,\n    \"ideal\": 0.26357391476631165,\n    \"perfect\": 0.2541669011116028,\n    \"##bing\": 0.24062614142894745,\n    \"material\": 0.23214206099510193,\n    \"storage\": 0.21676106750965118,\n    \"tool\": 0.21145488321781158,\n    \"nice\": 0.20749981701374054,\n    \"web\": 0.19741220772266388,\n    \"architecture\": 0.1926720291376114,\n    \"##b\": 0.18546289205551147,\n    \"book\": 0.18313883244991302,\n    \"a\": 0.17295610904693604,\n    \"speed\": 0.17008088529109955,\n    \"##am\": 0.1678074449300766,\n    \"##ization\": 0.16315481066703796,\n    \"browser\": 0.11985089629888535,\n    \"##ogen\": 0.10030396282672882,\n    \"database\": 0.09790635108947754,\n    \"connection\": 0.09682106971740723,\n    \"excellent\": 0.0670650377869606,\n    \"computer\": 0.06487759202718735,\n    \"java\": 0.055845409631729126,\n    \"algorithm\": 0.051508933305740356,\n    \"program\": 0.04257776960730553,\n    \"wonderful\": 0.00856015458703041\n}\n</code>\n</pre> <pre><code>def make_dense_embedding(texts: list[str]):\n    return list(dense_model.embed(texts))\n\n\ndense_embedding = make_dense_embedding([\"Fastembed is a great library for text embeddings!\"])\n</code></pre> <pre><code>dense_embedding[0].shape\n</code></pre> <pre>\n<code>(1024,)</code>\n</pre> <pre><code>product_texts = df[\"combined_text\"].tolist()\n</code></pre> <pre><code>%%time\ndf[\"sparse_embedding\"] = make_sparse_embedding(product_texts)\n</code></pre> <pre>\n<code>CPU times: user 5min 57s, sys: 22 s, total: 6min 19s\nWall time: 1min 37s\n</code>\n</pre> <p>Notice that FastEmbed uses data parallelism to speed up the embedding generation process. </p> <p>This improves throughput and reduces the time it takes to generate embeddings for large datasets. </p> <p>For our small dataset here, on my local machine -- it reduces the time from user's 6 min 15 seconds to a wall time of about 3 min 6 seconds, or about 2x faster. This is a function of the number of CPU cores available on the machine, CPU usage and other factors -- so your mileage may vary.</p> <pre><code>df[\"sparse_embedding\"]\n</code></pre> <pre>\n<code>0      SparseEmbedding(values=array([0.06509431, 0.57...\n2      SparseEmbedding(values=array([0.10595927, 0.20...\n3      SparseEmbedding(values=array([0.1140037 , 0.02...\n4      SparseEmbedding(values=array([6.13510251e-01, ...\n5      SparseEmbedding(values=array([0.90058267, 0.12...\n                             ...                        \n780    SparseEmbedding(values=array([5.56782305e-01, ...\n809    SparseEmbedding(values=array([0.38585788, 0.44...\n828    SparseEmbedding(values=array([3.27695787e-01, ...\n867    SparseEmbedding(values=array([0.36255798, 0.74...\n870    SparseEmbedding(values=array([3.74321818e-01, ...\nName: sparse_embedding, Length: 176, dtype: object</code>\n</pre> <pre><code>%%time\ndf[\"dense_embedding\"] = make_dense_embedding(product_texts)\n</code></pre> <pre>\n<code>CPU times: user 15min 51s, sys: 31.7 s, total: 16min 23s\nWall time: 3min\n</code>\n</pre> <pre><code>client = QdrantClient(\":memory:\")\n</code></pre> <pre><code>collection_name = \"esci\"\nclient.create_collection(\n    collection_name,\n    vectors_config={\n        \"text-dense\": VectorParams(\n            size=1024,  # OpenAI Embeddings\n            distance=Distance.COSINE,\n        )\n    },\n    sparse_vectors_config={\n        \"text-sparse\": SparseVectorParams(\n            index=SparseIndexParams(\n                on_disk=False,\n            )\n        )\n    },\n)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>def make_points(df: pd.DataFrame) -&amp;gt; list[PointStruct]:\n    sparse_vectors = df[\"sparse_embedding\"].tolist()\n    product_texts = df[\"combined_text\"].tolist()\n    dense_vectors = df[\"dense_embedding\"].tolist()\n    rows = df.to_dict(orient=\"records\")\n    points = []\n    for idx, (text, sparse_vector, dense_vector) in enumerate(\n        zip(product_texts, sparse_vectors, dense_vectors)\n    ):\n        sparse_vector = SparseVector(\n            indices=sparse_vector.indices.tolist(), values=sparse_vector.values.tolist()\n        )\n        point = PointStruct(\n            id=idx,\n            payload={\n                \"text\": text,\n                \"product_id\": rows[idx][\"product_id\"],\n            },  # Add any additional payload if necessary\n            vector={\n                \"text-sparse\": sparse_vector,\n                \"text-dense\": dense_vector.tolist(),\n            },\n        )\n        points.append(point)\n    return points\n\n\npoints: list[PointStruct] = make_points(df)\n</code></pre> <pre><code>client.upsert(collection_name, points)\n</code></pre> <pre>\n<code>UpdateResult(operation_id=0, status=&lt;UpdateStatus.COMPLETED: 'completed'&gt;)</code>\n</pre> <pre><code>def search(query_text: str):\n    # # Compute sparse and dense vectors\n    query_sparse_vectors: list[SparseEmbedding] = make_sparse_embedding([query_text])\n    query_dense_vector: list[np.ndarray] = make_dense_embedding([query_text])\n\n    search_results = client.search_batch(\n        collection_name=collection_name,\n        requests=[\n            SearchRequest(\n                vector=NamedVector(\n                    name=\"text-dense\",\n                    vector=query_dense_vector[0].tolist(),\n                ),\n                limit=10,\n                with_payload=True,\n            ),\n            SearchRequest(\n                vector=NamedSparseVector(\n                    name=\"text-sparse\",\n                    vector=SparseVector(\n                        indices=query_sparse_vectors[0].indices.tolist(),\n                        values=query_sparse_vectors[0].values.tolist(),\n                    ),\n                ),\n                limit=10,\n                with_payload=True,\n            ),\n        ],\n    )\n\n    return search_results\n\n\nquery_text = \" revent 80 cfm\"\nsearch_results = search(query_text)\n</code></pre> <pre><code>def rrf(rank_lists, alpha=60, default_rank=1000):\n    \"\"\"\n    Optimized Reciprocal Rank Fusion (RRF) using NumPy for large rank lists.\n\n    :param rank_lists: A list of rank lists. Each rank list should be a list of (item, rank) tuples.\n    :param alpha: The parameter alpha used in the RRF formula. Default is 60.\n    :param default_rank: The default rank assigned to items not present in a rank list. Default is 1000.\n    :return: Sorted list of items based on their RRF scores.\n    \"\"\"\n    # Consolidate all unique items from all rank lists\n    all_items = set(item for rank_list in rank_lists for item, _ in rank_list)\n\n    # Create a mapping of items to indices\n    item_to_index = {item: idx for idx, item in enumerate(all_items)}\n\n    # Initialize a matrix to hold the ranks, filled with the default rank\n    rank_matrix = np.full((len(all_items), len(rank_lists)), default_rank)\n\n    # Fill in the actual ranks from the rank lists\n    for list_idx, rank_list in enumerate(rank_lists):\n        for item, rank in rank_list:\n            rank_matrix[item_to_index[item], list_idx] = rank\n\n    # Calculate RRF scores using NumPy operations\n    rrf_scores = np.sum(1.0 / (alpha + rank_matrix), axis=1)\n\n    # Sort items based on RRF scores\n    sorted_indices = np.argsort(-rrf_scores)  # Negative for descending order\n\n    # Retrieve sorted items\n    sorted_items = [(list(item_to_index.keys())[idx], rrf_scores[idx]) for idx in sorted_indices]\n\n    return sorted_items\n\n\n# Example usage\nrank_list1 = [(\"A\", 1), (\"B\", 2), (\"C\", 3)]\nrank_list2 = [(\"B\", 1), (\"C\", 2), (\"D\", 3)]\nrank_list3 = [(\"A\", 2), (\"D\", 1), (\"E\", 3)]\n\n# Combine the rank lists\nsorted_items = rrf([rank_list1, rank_list2, rank_list3])\nsorted_items\n</code></pre> <pre>\n<code>[('A', 0.033465871107430434),\n ('B', 0.033465871107430434),\n ('D', 0.03320985472238179),\n ('C', 0.03294544435749548),\n ('E', 0.01775980832584606)]</code>\n</pre> <p>Based on this, let's convert our sparse and dense results into rank lists. And then, we'll use the Reciprocal Rank Fusion (RRF) algorithm to combine them.</p> <pre><code>def rank_list(search_result: list[ScoredPoint]):\n    return [(point.id, rank + 1) for rank, point in enumerate(search_result)]\n\n\ndense_rank_list, sparse_rank_list = rank_list(search_results[0]), rank_list(search_results[1])\n</code></pre> <pre><code>rrf_rank_list = rrf([dense_rank_list, sparse_rank_list])\n</code></pre> <pre><code>rrf_rank_list\n</code></pre> <pre>\n<code>[(3, 0.032018442622950824),\n (8, 0.03149801587301587),\n (1, 0.03131881575727918),\n (13, 0.030834914611005692),\n (15, 0.030536130536130537),\n (9, 0.030309988518943745),\n (12, 0.030158730158730156),\n (14, 0.029437229437229435),\n (11, 0.028985507246376812),\n (2, 0.01707242848447961),\n (4, 0.01564927857935627)]</code>\n</pre> <pre><code>def find_point_by_id(\n    client: QdrantClient, collection_name: str, rrf_rank_list: list[tuple[int, float]]\n):\n    return client.retrieve(\n        collection_name=collection_name, ids=[item[0] for item in rrf_rank_list]\n    )\n\n\nfind_point_by_id(client, collection_name, rrf_rank_list)\n</code></pre> <pre>\n<code>[Record(id=3, payload={'text': 'Delta Electronics RAD80L BreezRadiance 80 CFM Heater/Fan/Light Combo White (Renewed)\\nDelta Electronics RAD80L BreezRadiance 80 CFM Heater/Fan/Light Combo White (Renewed)\\nDELTA ELECTRONICS (AMERICAS) LTD.\\nWhite\\nThis pre-owned or refurbished product has been professionally inspected and tested to work and look like new. How a product becomes part of Amazon Renewed, your destination for pre-owned, refurbished products: A customer buys a new product and returns it or trades it in for a newer or different model. That product is inspected and tested to work and look like new by Amazon-qualified suppliers. Then, the product is sold as an Amazon Renewed product on Amazon. If not satisfied with the purchase, renewed products are eligible for replacement or refund under the Amazon Renewed Guarantee.\\nQuiet operation at 1.5 sones\\nBuilt-in thermostat regulates temperature. Energy efficiency at 7.6 CFM/Watt\\nPrecision engineered with DC brushless motor for extended reliability, this fan will outlast many household appliances\\nGalvanized steel construction resists corrosion\\nDuct: Detachable 4-inch Plastic Duct Adapter\\nQuiet operation at 1.5 sones\\nBuilt-in thermostat regulates temperature. Energy efficiency at 7.6 CFM/Watt\\nPrecision engineered with DC brushless motor for extended reliability, this fan will outlast many household appliances\\nGalvanized steel construction resists corrosion\\nDuct: Detachable 4-inch Plastic Duct Adapter', 'product_id': 'B07RH6Z8KW'}, vector=None, shard_key=None),\n Record(id=8, payload={'text': 'Aero Pure ABF80 L5 W ABF80L5 Ceiling Mount 80 CFM w/LED Light/Nightlight, Energy Star Certified, White Quiet Bathroom Ventilation Fan\\nAero Pure ABF80 L5 W ABF80L5 Ceiling Mount 80 CFM w/LED Light/Nightlight, Energy Star Certified, White Quiet Bathroom Ventilation Fan\\nAero Pure\\nWhite\\nNone\\nQuiet 0.3 Sones, 80 CFM fan with choice of three designer grilles in White, Satin Nickel, or Oil Rubbed Bronze; Full 6 year warranty\\n10W 3000K 800 Lumens LED Light with 0.7W Nightlight included\\nInstallation friendly- Quick-mount adjustable metal bracket for new construction and retrofit; 4\u201d, 5: and 6\u201d metal duct adaptor included\\nMeets today\u2019s demanding building specifications- ETL Listed for wet application, ENERGY STAR certified, CALGreen, JA-8 Compliant for CA Title 24, and ASHRAE 62.2 compliant\\nHousing dimensions- 10 2/5\u201dx10 2/5\u201dx 7 \u00bd\u201d; Grille dimensions- 13\u201dx13\u201d; Fits 2\"x8\" joists\\nQuiet 0.3 Sones, 80 CFM fan with choice of three designer grilles in White, Satin Nickel, or Oil Rubbed Bronze; Full 6 year warranty\\n10W 3000K 800 Lumens LED Light with 0.7W Nightlight included\\nInstallation friendly- Quick-mount adjustable metal bracket for new construction and retrofit; 4\u201d, 5: and 6\u201d metal duct adaptor included\\nMeets today\u2019s demanding building specifications- ETL Listed for wet application, ENERGY STAR certified, CALGreen, JA-8 Compliant for CA Title 24, and ASHRAE 62.2 compliant\\nHousing dimensions- 10 2/5\u201dx10 2/5\u201dx 7 \u00bd\u201d; Grille dimensions- 13\u201dx13\u201d; Fits 2\"x8\" joists', 'product_id': 'B07JY1PQNT'}, vector=None, shard_key=None),\n Record(id=1, payload={'text': \"Homewerks 7141-80 Bathroom Fan Integrated LED Light Ceiling Mount Exhaust Ventilation, 1.1 Sones, 80 CFM\\nHomewerks 7141-80 Bathroom Fan Integrated LED Light Ceiling Mount Exhaust Ventilation, 1.1 Sones, 80 CFM\\nHomewerks\\n80 CFM\\nNone\\nOUTSTANDING PERFORMANCE: This Homewerk's bath fan ensures comfort in your home by quietly eliminating moisture and humidity in the bathroom. This exhaust fan is 1.1 sones at 80 CFM which means it\u2019s able to manage spaces up to 80 square feet and is very quiet..\\nBATH FANS HELPS REMOVE HARSH ODOR: When cleaning the bathroom or toilet, harsh chemicals are used and they can leave an obnoxious odor behind. Homewerk\u2019s bathroom fans can help remove this odor with its powerful ventilation\\nBUILD QUALITY: Designed to be corrosion resistant with its galvanized steel construction featuring a modern style round shape and has an 4000K Cool White Light LED Light. AC motor.\\nEASY INSTALLATION: This exhaust bath fan is easy to install with its no-cut design and ceiling mount ventilation. Ceiling Opening (L) 7-1/2 in x Ceiling Opening (W) 7-1/4 x Ceiling Opening (H) 5-3/4 in. 13 in round grill and 4 in round duct connector.\\nHOMEWERKS TRUSTED QUALITY: Be confident in the quality and construction of each and every one of our products. We ensure that all of our products are produced and certified to regional, national and international industry standards. We are proud of the products we sell, you will be too. 3 Year Limited\\nOUTSTANDING PERFORMANCE: This Homewerk's bath fan ensures comfort in your home by quietly eliminating moisture and humidity in the bathroom. This exhaust fan is 1.1 sones at 80 CFM which means it\u2019s able to manage spaces up to 80 square feet and is very quiet..\\nBATH FANS HELPS REMOVE HARSH ODOR: When cleaning the bathroom or toilet, harsh chemicals are used and they can leave an obnoxious odor behind. Homewerk\u2019s bathroom fans can help remove this odor with its powerful ventilation\\nBUILD QUALITY: Designed to be corrosion resistant with its galvanized steel construction featuring a modern style round shape and has an 4000K Cool White Light LED Light. AC motor.\\nEASY INSTALLATION: This exhaust bath fan is easy to install with its no-cut design and ceiling mount ventilation. Ceiling Opening (L) 7-1/2 in x Ceiling Opening (W) 7-1/4 x Ceiling Opening (H) 5-3/4 in. 13 in round grill and 4 in round duct connector.\\nHOMEWERKS TRUSTED QUALITY: Be confident in the quality and construction of each and every one of our products. We ensure that all of our products are produced and certified to regional, national and international industry standards. We are proud of the products we sell, you will be too. 3 Year Limited\", 'product_id': 'B07X3Y6B1V'}, vector=None, shard_key=None),\n Record(id=13, payload={'text': 'Delta BreezSignature VFB25ACH 80 CFM Exhaust Bath Fan with Humidity Sensor\\nDelta BreezSignature VFB25ACH 80 CFM Exhaust Bath Fan with Humidity Sensor\\nDELTA ELECTRONICS (AMERICAS) LTD.\\nWhite\\nNone\\nVirtually silent at less than 0.3 sones\\nPrecision engineered with DC brushless motor for extended reliability\\nEasily switch in and out of humidity sensing mode by toggling wall switch\\nENERGY STAR qualified for efficient cost-saving operation\\nPrecision engineered with DC brushless motor for extended reliability, this fan will outlast many household appliances\\nVirtually silent at less than 0.3 sones\\nPrecision engineered with DC brushless motor for extended reliability\\nEasily switch in and out of humidity sensing mode by toggling wall switch\\nENERGY STAR qualified for efficient cost-saving operation\\nPrecision engineered with DC brushless motor for extended reliability, this fan will outlast many household appliances', 'product_id': 'B003O0MNGC'}, vector=None, shard_key=None),\n Record(id=15, payload={'text': 'Delta Electronics (Americas) Ltd. GBR80HLED Delta BreezGreenBuilder Series 80 CFM Fan/Dimmable H, LED Light, Dual Speed &amp; Humidity Sensor\\nDelta Electronics (Americas) Ltd. GBR80HLED Delta BreezGreenBuilder Series 80 CFM Fan/Dimmable H, LED Light, Dual Speed &amp; Humidity Sensor\\nDELTA ELECTRONICS (AMERICAS) LTD.\\nWith LED Light, Dual Speed &amp; Humidity Sensor\\nNone\\nUltra energy-efficient LED module (11-watt equivalent to 60-watt incandescent light) included. Main light output-850 Lumens, 3000K\\nExtracts air at a rate of 80 CFM to properly ventilate bathrooms up to 80 sq. Ft., quiet operation at 0.8 sones\\nPrecision engineered with DC brushless motor for extended reliability, this Fan will outlast many household appliances\\nEnergy Star qualified for efficient cost-saving operation, galvanized steel construction resists corrosion\\nFan impeller Stops If obstructed, for safe worry-free operation, attractive grille gives your bathroom a fresh look\\nUltra energy-efficient LED module (11-watt equivalent to 60-watt incandescent light) included. Main light output-850 Lumens, 3000K\\nExtracts air at a rate of 80 CFM to properly ventilate bathrooms up to 80 sq. Ft., quiet operation at 0.8 sones\\nPrecision engineered with DC brushless motor for extended reliability, this Fan will outlast many household appliances\\nEnergy Star qualified for efficient cost-saving operation, galvanized steel construction resists corrosion\\nFan impeller Stops If obstructed, for safe worry-free operation, attractive grille gives your bathroom a fresh look', 'product_id': 'B01N5Y6002'}, vector=None, shard_key=None),\n Record(id=9, payload={'text': \"Delta Electronics (Americas) Ltd. RAD80 Delta BreezRadiance Series 80 CFM Fan with Heater, 10.5W, 1.5 Sones\\nDelta Electronics (Americas) Ltd. RAD80 Delta BreezRadiance Series 80 CFM Fan with Heater, 10.5W, 1.5 Sones\\nDELTA ELECTRONICS (AMERICAS) LTD.\\nWith Heater\\nNone\\nQuiet operation at 1.5 Sones\\nPrecision engineered with DC brushless motor for extended reliability, this Fan will outlast many household appliances\\nGalvanized steel construction resists corrosion, equipped with metal duct adapter\\nFan impeller Stops If obstructed, for safe worry-free operation\\nPeace of mind quality, performance and reliability from the world's largest DC brushless Fan Manufacturer\\nQuiet operation at 1.5 Sones\\nPrecision engineered with DC brushless motor for extended reliability, this Fan will outlast many household appliances\\nGalvanized steel construction resists corrosion, equipped with metal duct adapter\\nFan impeller Stops If obstructed, for safe worry-free operation\\nPeace of mind quality, performance and reliability from the world's largest DC brushless Fan Manufacturer\", 'product_id': 'B01MZIK0PI'}, vector=None, shard_key=None),\n Record(id=12, payload={'text': 'Aero Pure AP80RVLW Super Quiet 80 CFM Recessed Fan/Light Bathroom Ventilation Fan with White Trim Ring\\nAero Pure AP80RVLW Super Quiet 80 CFM Recessed Fan/Light Bathroom Ventilation Fan with White Trim Ring\\nAero Pure\\nWhite\\nNone\\nSuper quiet 80CFM energy efficient fan virtually disappears into the ceiling leaving only a recessed light in view\\nMay be installed over shower when wired to a GFCI breaker and used with a PAR30L 75W (max) CFL\\nBulb not included. Accepts any of the following bulbs: 75W Max. PAR30, 14W Max. BR30 LED, or 75W Max. PAR30L (for use over tub/shower.)\\nSuper quiet 80CFM energy efficient fan virtually disappears into the ceiling leaving only a recessed light in view\\nMay be installed over shower when wired to a GFCI breaker and used with a PAR30L 75W (max) CFL\\nBulb not included. Accepts any of the following bulbs: 75W Max. PAR30, 14W Max. BR30 LED, or 75W Max. PAR30L (for use over tub/shower.)', 'product_id': 'B00MARNO5Y'}, vector=None, shard_key=None),\n Record(id=14, payload={'text': 'Broan Very Quiet Ceiling Bathroom Exhaust Fan, ENERGY STAR Certified, 0.3 Sones, 80 CFM\\nBroan Very Quiet Ceiling Bathroom Exhaust Fan, ENERGY STAR Certified, 0.3 Sones, 80 CFM\\nBroan-NuTone\\nWhite\\nNone\\nHIGH-QUALITY FAN: Very quiet, energy efficient exhaust fan runs on 0. 3 Sones and is motor engineered for continuous operation\\nEFFICIENT: Operates at 80 CFM in bathrooms up to 75 sq. ft. for a high-quality performance. Dimmable Capability: Non Dimmable\\nEASY INSTALLATION: Fan is easy to install and/or replace existing product for DIY\\'ers and needs only 2\" x 8\" construction space. Can be used over bathtubs or showers when connected to a GFCI protected branch circuit\\nFEATURES: Includes hanger bar system for fast, flexible installation for all types of construction and a 6\" ducting for superior performance\\nCERTIFIED: ENERGY STAR qualified and HVI Certified to ensure the best quality for your home\\nHIGH-QUALITY FAN: Very quiet, energy efficient exhaust fan runs on 0. 3 Sones and is motor engineered for continuous operation\\nEFFICIENT: Operates at 80 CFM in bathrooms up to 75 sq. ft. for a high-quality performance. Dimmable Capability: Non Dimmable\\nEASY INSTALLATION: Fan is easy to install and/or replace existing product for DIY\\'ers and needs only 2\" x 8\" construction space. Can be used over bathtubs or showers when connected to a GFCI protected branch circuit\\nFEATURES: Includes hanger bar system for fast, flexible installation for all types of construction and a 6\" ducting for superior performance\\nCERTIFIED: ENERGY STAR qualified and HVI Certified to ensure the best quality for your home', 'product_id': 'B001E6DMKY'}, vector=None, shard_key=None),\n Record(id=11, payload={'text': 'Panasonic FV-0811VF5 WhisperFit EZ Retrofit Ventilation Fan, 80 or 110 CFM\\nPanasonic FV-0811VF5 WhisperFit EZ Retrofit Ventilation Fan, 80 or 110 CFM\\nPanasonic\\nWhite\\nNone\\nRetrofit Solution: Ideal for residential remodeling, hotel construction or renovations\\nLow Profile: 5-5/8-Inch housing depth fits in a 2 x 6 construction\\nPick-A-Flow Speed Selector: Allows you to pick desired airflow from 80 or 110 CFM\\nFlexible Installation: Comes with Flex-Z Fast bracket for easy, fast and trouble-free installation\\nEnergy Star Rated: Delivers powerful airflow without wasting energy\\nRetrofit Solution: Ideal for residential remodeling, hotel construction or renovations\\nLow Profile: 5-5/8-Inch housing depth fits in a 2 x 6 construction\\nPick-A-Flow Speed Selector: Allows you to pick desired airflow from 80 or 110 CFM\\nFlexible Installation: Comes with Flex-Z Fast bracket for easy, fast and trouble-free installation\\nEnergy Star Rated: Delivers powerful airflow without wasting energy', 'product_id': 'B00XBZFWWM'}, vector=None, shard_key=None),\n Record(id=2, payload={'text': 'Homewerks 7140-80 Bathroom Fan Ceiling Mount Exhaust Ventilation, 1.5 Sones, 80 CFM, White\\nHomewerks 7140-80 Bathroom Fan Ceiling Mount Exhaust Ventilation, 1.5 Sones, 80 CFM, White\\nHomewerks\\nWhite\\nNone\\nOUTSTANDING PERFORMANCE: This Homewerk\\'s bath fan ensures comfort in your home by quietly eliminating moisture and humidity in the bathroom. This exhaust fan is 1. 5 sone at 110 CFM which means it\u2019s able to manage spaces up to 110 square feet\\nBATH FANS HELPS REMOVE HARSH ODOR: When cleaning the bathroom or toilet, harsh chemicals are used and they can leave an obnoxious odor behind. Homewerk\u2019s bathroom fans can help remove this odor with its powerful ventilation\\nBUILD QUALITY: Designed to be corrosion resistant with its galvanized steel construction featuring a grille modern style.\\nEASY INSTALLATION: This exhaust bath fan is easy to install with its no-cut design and ceiling mount ventilation. Ceiling Opening (L) 7-1/2 in x Ceiling Opening (W) 7-1/4 x Ceiling Opening (H) 5-3/4 in and a 4\" round duct connector.\\nHOMEWERKS TRUSTED QUALITY: Be confident in the quality and construction of each and every one of our products. We ensure that all of our products are produced and certified to regional, national and international industry standards. We are proud of the products we sell, you will be too. 3 Year Limited\\nOUTSTANDING PERFORMANCE: This Homewerk\\'s bath fan ensures comfort in your home by quietly eliminating moisture and humidity in the bathroom. This exhaust fan is 1. 5 sone at 110 CFM which means it\u2019s able to manage spaces up to 110 square feet\\nBATH FANS HELPS REMOVE HARSH ODOR: When cleaning the bathroom or toilet, harsh chemicals are used and they can leave an obnoxious odor behind. Homewerk\u2019s bathroom fans can help remove this odor with its powerful ventilation\\nBUILD QUALITY: Designed to be corrosion resistant with its galvanized steel construction featuring a grille modern style.\\nEASY INSTALLATION: This exhaust bath fan is easy to install with its no-cut design and ceiling mount ventilation. Ceiling Opening (L) 7-1/2 in x Ceiling Opening (W) 7-1/4 x Ceiling Opening (H) 5-3/4 in and a 4\" round duct connector.\\nHOMEWERKS TRUSTED QUALITY: Be confident in the quality and construction of each and every one of our products. We ensure that all of our products are produced and certified to regional, national and international industry standards. We are proud of the products we sell, you will be too. 3 Year Limited', 'product_id': 'B07WDM7MQQ'}, vector=None, shard_key=None),\n Record(id=4, payload={'text': 'Panasonic FV-08VRE2 Ventilation Fan with Recessed LED (Renewed)\\nPanasonic FV-08VRE2 Ventilation Fan with Recessed LED (Renewed)\\nPanasonic\\nWhite\\nNone\\nThe design solution for Fan/light combinations\\nEnergy Star rated architectural grade recessed Fan/LED light\\nQuiet, energy efficient and powerful 80 CFM ventilation hidden above the Ceiling\\nLED lamp is dimmable\\nBeautiful Lighting with 6-1/2\u201daperture and advanced luminaire design\\nThe design solution for Fan/light combinations\\nEnergy Star rated architectural grade recessed Fan/LED light\\nQuiet, energy efficient and powerful 80 CFM ventilation hidden above the Ceiling\\nLED lamp is dimmable\\nBeautiful Lighting with 6-1/2\u201daperture and advanced luminaire design', 'product_id': 'B07QJ7WYFQ'}, vector=None, shard_key=None)]</code>\n</pre> <p>Next, let's check the ESCI (Exact, Substitute, Compliment, and Irrelvant) label for the results against the source data.</p> <pre><code>ids = [item[0] for item in rrf_rank_list]\ndf[df[\"query\"] == query_text]\n\nfor idx in ids:\n    print(df.iloc[idx][\"esci_label\"])\n</code></pre> <pre>\n<code>Exact\nExact\nExact\nExact\nExact\nExact\nExact\nExact\nExact\nExact\nExact\n</code>\n</pre> <p>This was amazing! We pulled only Exact results with k=10. This is a great result for a small dataset like this with out of the box vectors which are not even fine-tuned for e-Commerce.</p> <pre><code>len(rrf_rank_list)\n</code></pre> <pre>\n<code>11</code>\n</pre>"},{"location":"examples/Hybrid_Search/#hybrid-search-with-fastembed-qdrant","title":"Hybrid Search with FastEmbed &amp; Qdrant","text":"<p>Author: Nirant Kasliwal</p>"},{"location":"examples/Hybrid_Search/#what-will-we-do","title":"What will we do?","text":"<p>This notebook demonstrates the usage of Hybrid Search with FastEmbed &amp; Qdrant. </p> <ol> <li>Setup: Download and install the required dependencies</li> <li>Preview data: Load and preview the data</li> <li>Create Sparse Embeddings: Create SPLADE++ embeddings for the data</li> <li>Create Dense Embeddings: Create BGE-Large-en-v1.5 embeddings for the data</li> <li>Indexing: Index the embeddings using Qdrant</li> <li>Search: Perform Hybrid Search using FastEmbed &amp; Qdrant</li> <li>Ranking: Rank the search results with Reciprocal Rank Fusion (RRF)</li> </ol>"},{"location":"examples/Hybrid_Search/#setup","title":"Setup","text":"<p>In order to get started, you need a few dependencies, and we'll install them next:</p>"},{"location":"examples/Hybrid_Search/#preview-data","title":"Preview Data","text":""},{"location":"examples/Hybrid_Search/#create-sparse-embeddings","title":"Create Sparse Embeddings","text":""},{"location":"examples/Hybrid_Search/#create-dense-embeddings","title":"Create Dense Embeddings","text":""},{"location":"examples/Hybrid_Search/#indexing","title":"Indexing","text":""},{"location":"examples/Hybrid_Search/#about-qdrant","title":"About Qdrant","text":"<p>Qdrant is a vector similarity search engine that allows you to index and search high-dimensional vectors. It supports both sparse and dense embeddings, and it's a great tool for building search engines. </p> <p>Here, we use the memory mode which is Numpy under the hood for demonstration purposes. In production, you can use the Docker or Cloud for full DB support.</p>"},{"location":"examples/Hybrid_Search/#search","title":"Search","text":""},{"location":"examples/Hybrid_Search/#ranking","title":"Ranking","text":"<p>We'll combine the results from the two models using Reciprocal Rank Fusion (RRF). You can read more about RRF here.</p> <p>We select RRF for this task because: 1. It is a simple and effective method for combining search results. 2. It is robust to the differences in the ranking scores of the two or more ranking lists. 3. It is easy to implement and requires minimal tuning (only one parameter: alpha, which we don't tune here).</p>"},{"location":"examples/Hybrid_Search/#conclusion","title":"Conclusion","text":"<p>In this notebook, we demonstrated the usage of Hybrid Search with FastEmbed &amp; Qdrant. We used FastEmbed to create Sparse and Dense embeddings for the data and indexed them using Qdrant. We then performed Hybrid Search using FastEmbed &amp; Qdrant and ranked the search results using Reciprocal Rank Fusion (RRF).</p>"},{"location":"examples/Image_Embedding/","title":"Image Embedding","text":"<pre><code>from fastembed import ImageEmbedding\n\nmodel = ImageEmbedding(\"Qdrant/resnet50-onnx\")\n\nembeddings_generator = model.embed(\n    [\"../../tests/misc/image.jpeg\", \"../../tests/misc/small_image.jpeg\"]\n)\nembeddings_list = list(embeddings_generator)\nembeddings_list\n</code></pre> <pre>\n<code>Fetching 3 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00&lt;00:00, 47482.69it/s]\n</code>\n</pre> <pre>\n<code>[array([0.        , 0.        , 0.        , ..., 0.        , 0.01139933,\n        0.        ], dtype=float32),\n array([0.02169187, 0.        , 0.        , ..., 0.        , 0.00848291,\n        0.        ], dtype=float32)]</code>\n</pre> <pre><code>ImageEmbedding.list_supported_models()\n</code></pre> <pre>\n<code>[{'model': 'Qdrant/clip-ViT-B-32-vision',\n  'dim': 512,\n  'description': 'CLIP vision encoder based on ViT-B/32',\n  'size_in_GB': 0.34,\n  'sources': {'hf': 'Qdrant/clip-ViT-B-32-vision'},\n  'model_file': 'model.onnx'},\n {'model': 'Qdrant/resnet50-onnx',\n  'dim': 2048,\n  'description': 'ResNet-50 from `Deep Residual Learning for Image Recognition &lt;https://arxiv.org/abs/1512.03385&gt;`__.',\n  'size_in_GB': 0.1,\n  'sources': {'hf': 'Qdrant/resnet50-onnx'},\n  'model_file': 'model.onnx'}]</code>\n</pre>"},{"location":"examples/Image_Embedding/#image-embedding","title":"Image Embedding","text":"<p>As of version 0.3.0 fastembed supports computation of image embeddings.</p> <p>The process is as easy and straightforward as with text embeddings. Let's see how it works.</p>"},{"location":"examples/Image_Embedding/#preprocessing","title":"Preprocessing","text":"<p>Preprocessing is encapsulated in the ImageEmbedding class, applied operations are identical to the ones provided by Hugging Face Transformers. You don't need to think about batching, opening/closing files, resizing images, etc., Fastembed will take care of it.</p>"},{"location":"examples/Image_Embedding/#supported-models","title":"Supported models","text":"<p>List of supported image embedding models can either be found here or by calling the <code>ImageEmbedding.list_supported_models()</code> method.</p>"},{"location":"examples/SPLADE_with_FastEmbed/","title":"SPLADE with FastEmbed","text":"<pre><code># !pip install -q fastembed\n</code></pre> <p>Let's get started! \ud83d\ude80</p> <pre><code>from fastembed import SparseTextEmbedding, SparseEmbedding\n</code></pre> <p>&gt; You can find the list of all supported Sparse Embedding models by calling this API: <code>SparseTextEmbedding.list_supported_models()</code></p> <pre><code>SparseTextEmbedding.list_supported_models()\n</code></pre> <pre>\n<code>[{'model': 'prithvida/Splade_PP_en_v1',\n  'vocab_size': 30522,\n  'description': 'Misspelled version of the model. Retained for backward compatibility. Independent Implementation of SPLADE++ Model for English',\n  'size_in_GB': 0.532,\n  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}},\n {'model': 'prithivida/Splade_PP_en_v1',\n  'vocab_size': 30522,\n  'description': 'Independent Implementation of SPLADE++ Model for English',\n  'size_in_GB': 0.532,\n  'sources': {'hf': 'Qdrant/SPLADE_PP_en_v1'}}]</code>\n</pre> <pre><code>model_name = \"prithvida/Splade_PP_en_v1\"\n# This triggers the model download\nmodel = SparseTextEmbedding(model_name=model_name)\n</code></pre> <pre>\n<code>Fetching 9 files:   0%|          | 0/9 [00:00&lt;?, ?it/s]</code>\n</pre> <pre><code>documents: list[str] = [\n    \"Chandrayaan-3 is India's third lunar mission\",\n    \"It aimed to land a rover on the Moon's surface - joining the US, China and Russia\",\n    \"The mission is a follow-up to Chandrayaan-2, which had partial success\",\n    \"Chandrayaan-3 will be launched by the Indian Space Research Organisation (ISRO)\",\n    \"The estimated cost of the mission is around $35 million\",\n    \"It will carry instruments to study the lunar surface and atmosphere\",\n    \"Chandrayaan-3 landed on the Moon's surface on 23rd August 2023\",\n    \"It consists of a lander named Vikram and a rover named Pragyan similar to Chandrayaan-2. Its propulsion module would act like an orbiter.\",\n    \"The propulsion module carries the lander and rover configuration until the spacecraft is in a 100-kilometre (62 mi) lunar orbit\",\n    \"The mission used GSLV Mk III rocket for its launch\",\n    \"Chandrayaan-3 was launched from the Satish Dhawan Space Centre in Sriharikota\",\n    \"Chandrayaan-3 was launched earlier in the year 2023\",\n]\nsparse_embeddings_list: list[SparseEmbedding] = list(\n    model.embed(documents, batch_size=6)\n)  # batch_size is optional, notice the generator\n</code></pre> <pre><code>index = 0\nsparse_embeddings_list[index]\n</code></pre> <pre>\n<code>SparseEmbedding(values=array([0.05297208, 0.01963477, 0.36459631, 1.38508618, 0.71776593,\n       0.12667948, 0.46230844, 0.446771  , 0.26897505, 1.01519883,\n       1.5655334 , 0.29412213, 1.53102326, 0.59785569, 1.1001817 ,\n       0.02079751, 0.09955651, 0.44249091, 0.09747757, 1.53519952,\n       1.36765671, 0.15740395, 0.49882549, 0.38629025, 0.76612782,\n       1.25805044, 0.39058095, 0.27236196, 0.45152301, 0.48262018,\n       0.26085234, 1.35912788, 0.70710695, 1.71639752]), indices=array([ 1010,  1011,  1016,  1017,  2001,  2018,  2034,  2093,  2117,\n        2319,  2353,  2509,  2634,  2686,  2796,  2817,  2922,  2959,\n        3003,  3148,  3260,  3390,  3462,  3523,  3822,  4231,  4316,\n        4774,  5590,  5871,  6416, 11926, 12076, 16469]))</code>\n</pre> <p>The previous output is a SparseEmbedding object for the first document in our list.</p> <p>It contains two arrays: values and indices.  - The 'values' array represents the weights of the features (tokens) in the document. - The 'indices' array represents the indices of these features in the model's vocabulary.</p> <p>Each pair of corresponding values and indices represents a token and its weight in the document.</p> <pre><code># Let's print the first 5 features and their weights for better understanding.\nfor i in range(5):\n    print(\n        f\"Token at index {sparse_embeddings_list[0].indices[i]} has weight {sparse_embeddings_list[0].values[i]}\"\n    )\n</code></pre> <pre>\n<code>Token at index 1010 has weight 0.05297207832336426\nToken at index 1011 has weight 0.01963476650416851\nToken at index 1016 has weight 0.36459630727767944\nToken at index 1017 has weight 1.385086178779602\nToken at index 2001 has weight 0.7177659273147583\n</code>\n</pre> <pre><code>import json\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\n    SparseTextEmbedding.list_supported_models()[0][\"sources\"][\"hf\"]\n)\n</code></pre> <pre><code>def get_tokens_and_weights(sparse_embedding, tokenizer):\n    token_weight_dict = {}\n    for i in range(len(sparse_embedding.indices)):\n        token = tokenizer.decode([sparse_embedding.indices[i]])\n        weight = sparse_embedding.values[i]\n        token_weight_dict[token] = weight\n\n    # Sort the dictionary by weights\n    token_weight_dict = dict(\n        sorted(token_weight_dict.items(), key=lambda item: item[1], reverse=True)\n    )\n    return token_weight_dict\n\n\n# Test the function with the first SparseEmbedding\nprint(json.dumps(get_tokens_and_weights(sparse_embeddings_list[index], tokenizer), indent=4))\n</code></pre> <pre>\n<code>{\n    \"chandra\": 1.7163975238800049,\n    \"third\": 1.5655333995819092,\n    \"##ya\": 1.535199522972107,\n    \"india\": 1.5310232639312744,\n    \"3\": 1.385086178779602,\n    \"mission\": 1.3676567077636719,\n    \"lunar\": 1.3591278791427612,\n    \"moon\": 1.2580504417419434,\n    \"indian\": 1.1001816987991333,\n    \"##an\": 1.015198826789856,\n    \"3rd\": 0.7661278247833252,\n    \"was\": 0.7177659273147583,\n    \"spacecraft\": 0.7071069478988647,\n    \"space\": 0.5978556871414185,\n    \"flight\": 0.4988254904747009,\n    \"satellite\": 0.4826201796531677,\n    \"first\": 0.46230843663215637,\n    \"expedition\": 0.4515230059623718,\n    \"three\": 0.4467709958553314,\n    \"fourth\": 0.44249090552330017,\n    \"vehicle\": 0.390580952167511,\n    \"iii\": 0.3862902522087097,\n    \"2\": 0.36459630727767944,\n    \"##3\": 0.2941221296787262,\n    \"planet\": 0.27236196398735046,\n    \"second\": 0.26897504925727844,\n    \"missions\": 0.2608523368835449,\n    \"launched\": 0.15740394592285156,\n    \"had\": 0.12667948007583618,\n    \"largest\": 0.09955651313066483,\n    \"leader\": 0.09747757017612457,\n    \",\": 0.05297207832336426,\n    \"study\": 0.02079751156270504,\n    \"-\": 0.01963476650416851\n}\n</code>\n</pre> <pre><code>\n</code></pre>"},{"location":"examples/SPLADE_with_FastEmbed/#introduction-to-splade-with-fastembed","title":"Introduction to SPLADE with FastEmbed","text":"<p>In this notebook, we will explore how to generate Sparse Vectors -- in particular a variant of the SPLADE.</p> <p>&gt; \ud83d\udca1 The original naver/SPLADE models were licensed CC BY-NC-SA 4.0 -- Not for Commercial Use. This SPLADE++ model is Apache License and hence, licensed for commercial use. </p>"},{"location":"examples/SPLADE_with_FastEmbed/#outline","title":"Outline:","text":"<ol> <li>What is SPLADE?</li> <li>Setting up the environment</li> <li>Generating SPLADE vectors with FastEmbed</li> <li>Understanding SPLADE vectors</li> <li>Observations and Design Choices</li> </ol>"},{"location":"examples/SPLADE_with_FastEmbed/#what-is-splade","title":"What is SPLADE?","text":"<p>SPLADE was a novel method for learning sparse vectors for text representation. This model beats BM25 -- the underlying approach for the Elastic/Lucene family of implementations. Thus making it highly effective for tasks such as information retrieval, document classification, and more. </p> <p>The key advantage of SPLADE is its ability to generate sparse vectors, which are more efficient and interpretable than dense vectors. This makes SPLADE a powerful tool for handling large-scale text data.</p>"},{"location":"examples/SPLADE_with_FastEmbed/#setting-up-the-environment","title":"Setting up the environment","text":"<p>This notebook uses few dependencies, which are installed below: </p>"},{"location":"examples/SPLADE_with_FastEmbed/#understanding-splade-vectors","title":"Understanding SPLADE vectors","text":"<p>This is still a little abstract, so let's use the tokenizer vocab to make sense of these indices.</p>"},{"location":"examples/SPLADE_with_FastEmbed/#observations-and-model-design-choices","title":"Observations and Model Design Choices","text":"<ol> <li>The relative order of importance is quite useful. The most important tokens in the sentence have the highest weights.</li> <li>Term Expansion: The model can expand the terms in the document. This means that the model can generate weights for tokens that are not present in the document but are related to the tokens in the document. This is a powerful feature that allows the model to capture the context of the document. Here, you'll see that the model has added the tokens '3' from 'third' and 'moon' from 'lunar' to the sparse vector.</li> </ol>"},{"location":"examples/SPLADE_with_FastEmbed/#design-choices","title":"Design Choices","text":"<ol> <li>The weights are not normalized. This means that the sum of the weights is not 1 or 100. This is a common practice in sparse embeddings, as it allows the model to capture the importance of each token in the document.</li> <li>Tokens are included in the sparse vector only if they are present in the model's vocabulary. This means that the model will not generate a weight for tokens that it has not seen during training.</li> <li>Tokens do not map to words directly -- allowing you to gracefully handle typo errors and out-of-vocabulary tokens.</li> </ol>"},{"location":"examples/Supported_Models/","title":"Supported Models","text":"<pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <pre>\n<code>The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n</code>\n</pre> <pre><code>import pandas as pd\n\nfrom fastembed import (\n    SparseTextEmbedding,\n    TextEmbedding,\n    LateInteractionTextEmbedding,\n    ImageEmbedding,\n)\nfrom fastembed.rerank.cross_encoder import TextCrossEncoder\n</code></pre> <pre><code>supported_models = (\n    pd.DataFrame(TextEmbedding.list_supported_models())\n    .sort_values(\"size_in_GB\")\n    .drop(columns=[\"sources\", \"model_file\", \"additional_files\"])\n    .reset_index(drop=True)\n)\nsupported_models\n</code></pre> model dim description license size_in_GB 0 BAAI/bge-small-en-v1.5 384 Text embeddings, Unimodal (text), English, 512... mit 0.067 1 BAAI/bge-small-zh-v1.5 512 Text embeddings, Unimodal (text), Chinese, 512... mit 0.090 2 snowflake/snowflake-arctic-embed-xs 384 Text embeddings, Unimodal (text), English, 512... apache-2.0 0.090 3 sentence-transformers/all-MiniLM-L6-v2 384 Text embeddings, Unimodal (text), English, 256... apache-2.0 0.090 4 jinaai/jina-embeddings-v2-small-en 512 Text embeddings, Unimodal (text), English, 819... apache-2.0 0.120 5 BAAI/bge-small-en 384 Text embeddings, Unimodal (text), English, 512... mit 0.130 6 snowflake/snowflake-arctic-embed-s 384 Text embeddings, Unimodal (text), English, 512... apache-2.0 0.130 7 nomic-ai/nomic-embed-text-v1.5-Q 768 Text embeddings, Multimodal (text, image), Eng... apache-2.0 0.130 8 BAAI/bge-base-en-v1.5 768 Text embeddings, Unimodal (text), English, 512... mit 0.210 9 sentence-transformers/paraphrase-multilingual-... 384 Text embeddings, Unimodal (text), Multilingual... apache-2.0 0.220 10 Qdrant/clip-ViT-B-32-text 512 Text embeddings, Multimodal (text&amp;image), Engl... mit 0.250 11 jinaai/jina-embeddings-v2-base-de 768 Text embeddings, Unimodal (text), Multilingual... apache-2.0 0.320 12 BAAI/bge-base-en 768 Text embeddings, Unimodal (text), English, 512... mit 0.420 13 snowflake/snowflake-arctic-embed-m 768 Text embeddings, Unimodal (text), English, 512... apache-2.0 0.430 14 nomic-ai/nomic-embed-text-v1.5 768 Text embeddings, Multimodal (text, image), Eng... apache-2.0 0.520 15 jinaai/jina-embeddings-v2-base-en 768 Text embeddings, Unimodal (text), English, 819... apache-2.0 0.520 16 nomic-ai/nomic-embed-text-v1 768 Text embeddings, Multimodal (text, image), Eng... apache-2.0 0.520 17 snowflake/snowflake-arctic-embed-m-long 768 Text embeddings, Unimodal (text), English, 204... apache-2.0 0.540 18 mixedbread-ai/mxbai-embed-large-v1 1024 Text embeddings, Unimodal (text), English, 512... apache-2.0 0.640 19 jinaai/jina-embeddings-v2-base-code 768 Text embeddings, Unimodal (text), Multilingual... apache-2.0 0.640 20 sentence-transformers/paraphrase-multilingual-... 768 Text embeddings, Unimodal (text), Multilingual... apache-2.0 1.000 21 snowflake/snowflake-arctic-embed-l 1024 Text embeddings, Unimodal (text), English, 512... apache-2.0 1.020 22 thenlper/gte-large 1024 Text embeddings, Unimodal (text), English, 512... mit 1.200 23 BAAI/bge-large-en-v1.5 1024 Text embeddings, Unimodal (text), English, 512... mit 1.200 24 intfloat/multilingual-e5-large 1024 Text embeddings, Unimodal (text), Multilingual... mit 2.240 <pre><code>(\n    pd.DataFrame(SparseTextEmbedding.list_supported_models())\n    .sort_values(\"size_in_GB\")\n    .drop(columns=[\"sources\", \"model_file\", \"additional_files\"])\n    .reset_index(drop=True)\n)\n</code></pre> model vocab_size description license size_in_GB requires_idf 0 Qdrant/bm25 NaN BM25 as sparse embeddings meant to be used wit... apache-2.0 0.010 True 1 Qdrant/bm42-all-minilm-l6-v2-attentions 30522.0 Light sparse embedding model, which assigns an... apache-2.0 0.090 True 2 prithivida/Splade_PP_en_v1 30522.0 Independent Implementation of SPLADE++ Model f... apache-2.0 0.532 NaN 3 prithvida/Splade_PP_en_v1 30522.0 Independent Implementation of SPLADE++ Model f... apache-2.0 0.532 NaN <pre><code>(\n    pd.DataFrame(LateInteractionTextEmbedding.list_supported_models())\n    .sort_values(\"size_in_GB\")\n    .drop(columns=[\"sources\", \"model_file\"])\n    .reset_index(drop=True)\n)\n</code></pre> model dim description license size_in_GB additional_files 0 answerdotai/answerai-colbert-small-v1 96 Text embeddings, Unimodal (text), Multilingual... apache-2.0 0.13 NaN 1 colbert-ir/colbertv2.0 128 Late interaction model mit 0.44 NaN 2 jinaai/jina-colbert-v2 128 New model that expands capabilities of colbert... cc-by-nc-4.0 2.24 [onnx/model.onnx_data] <pre><code>(\n    pd.DataFrame(ImageEmbedding.list_supported_models())\n    .sort_values(\"size_in_GB\")\n    .drop(columns=[\"sources\", \"model_file\"])\n    .reset_index(drop=True)\n)\n</code></pre> model dim description license size_in_GB 0 Qdrant/resnet50-onnx 2048 Image embeddings, Unimodal (image), 2016 year apache-2.0 0.10 1 Qdrant/clip-ViT-B-32-vision 512 Image embeddings, Multimodal (text&amp;image), 202... mit 0.34 2 Qdrant/Unicom-ViT-B-32 512 Image embeddings, Multimodal (text&amp;image), 202... apache-2.0 0.48 3 Qdrant/Unicom-ViT-B-16 768 Image embeddings (more detailed than Unicom-Vi... apache-2.0 0.82 <pre><code>(\n    pd.DataFrame(TextCrossEncoder.list_supported_models())\n    .sort_values(\"size_in_GB\")\n    .drop(columns=[\"sources\", \"model_file\"])\n    .reset_index(drop=True)\n)\n</code></pre> model size_in_GB description license 0 Xenova/ms-marco-MiniLM-L-6-v2 0.08 MiniLM-L-6-v2 model optimized for re-ranking t... apache-2.0 1 Xenova/ms-marco-MiniLM-L-12-v2 0.12 MiniLM-L-12-v2 model optimized for re-ranking ... apache-2.0 2 jinaai/jina-reranker-v1-tiny-en 0.13 Designed for blazing-fast re-ranking with 8K c... apache-2.0 3 jinaai/jina-reranker-v1-turbo-en 0.15 Designed for blazing-fast re-ranking with 8K c... apache-2.0 4 BAAI/bge-reranker-base 1.04 BGE reranker base model for cross-encoder re-r... mit 5 jinaai/jina-reranker-v2-base-multilingual 1.11 A multi-lingual reranker model for cross-encod... cc-by-nc-4.0 <pre><code>\n</code></pre>"},{"location":"examples/Supported_Models/#supported-text-embedding-models","title":"Supported Text Embedding Models","text":""},{"location":"examples/Supported_Models/#supported-sparse-text-embedding-models","title":"Supported Sparse Text Embedding Models","text":""},{"location":"examples/Supported_Models/#supported-late-interaction-text-embedding-models","title":"Supported Late Interaction Text Embedding Models","text":""},{"location":"examples/Supported_Models/#supported-image-embedding-models","title":"Supported Image Embedding Models","text":""},{"location":"examples/Supported_Models/#supported-rerank-cross-encoder-models","title":"Supported Rerank Cross Encoder Models","text":""},{"location":"experimental/Binary%20Quantization%20from%20Scratch/","title":"Binary Quantization from Scratch","text":"<pre><code>!pip install matplotlib tqdm pandas numpy datasets --quiet --upgrade\n</code></pre> <pre><code>import numpy as np\nimport pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n</code></pre> <pre><code># Download from Huggingface Hub\nds = load_dataset(\n    \"Qdrant/dbpedia-entities-openai3-text-embedding-3-large-3072-100K\", split=\"train\"\n)\nopenai_vectors = np.array(ds[\"text-embedding-3-large-3072-embedding\"])\ndel ds\n</code></pre> <pre><code>openai_bin = np.zeros_like(openai_vectors, dtype=np.int8)\nopenai_bin[openai_vectors &amp;gt; 0] = 1\n</code></pre> <pre><code>n_dim = openai_vectors.shape[1]\nn_dim\n</code></pre> <pre>\n<code>3072</code>\n</pre> <pre><code>def accuracy(idx, limit: int, oversampling: int):\n    scores = np.dot(openai_vectors, openai_vectors[idx])\n    dot_results = np.argsort(scores)[-limit:][::-1]\n\n    bin_scores = n_dim - np.logical_xor(openai_bin, openai_bin[idx]).sum(axis=1)\n    bin_results = np.argsort(bin_scores)[-(limit * oversampling) :][::-1]\n\n    return len(set(dot_results).intersection(set(bin_results))) / limit\n</code></pre> <pre><code>number_of_samples = 10\nlimits = [3, 10]\nsampling_rate = [1, 2, 3, 5]\nresults = []\n\n\ndef mean_accuracy(number_of_samples, limit, sampling_rate):\n    return np.mean(\n        [accuracy(i, limit=limit, oversampling=sampling_rate) for i in range(number_of_samples)]\n    )\n\n\nfor i in tqdm(sampling_rate):\n    for j in tqdm(limits):\n        result = {\n            \"sampling_rate\": i,\n            \"limit\": j,\n            \"mean_acc\": mean_accuracy(number_of_samples, j, i),\n        }\n        print(result)\n        results.append(result)\n</code></pre> <pre>\n<code>  0%|          | 0/4 [00:00&lt;?, ?it/s]\n  0%|          | 0/2 [00:00&lt;?, ?it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:02&lt;00:02,  2.05s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 1, 'limit': 3, 'mean_acc': 0.9}\n</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04&lt;00:00,  2.02s/it]\n 25%|\u2588\u2588\u258c       | 1/4 [00:04&lt;00:12,  4.05s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 1, 'limit': 10, 'mean_acc': 0.8300000000000001}\n</code>\n</pre> <pre>\n<code>\n  0%|          | 0/2 [00:00&lt;?, ?it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:01&lt;00:01,  1.72s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 2, 'limit': 3, 'mean_acc': 1.0}\n</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03&lt;00:00,  1.76s/it]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 2/4 [00:07&lt;00:07,  3.75s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 2, 'limit': 10, 'mean_acc': 0.9700000000000001}\n</code>\n</pre> <pre>\n<code>\n  0%|          | 0/2 [00:00&lt;?, ?it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:01&lt;00:01,  1.72s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 3, 'limit': 3, 'mean_acc': 1.0}\n</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03&lt;00:00,  1.69s/it]\n 75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c  | 3/4 [00:10&lt;00:03,  3.58s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 3, 'limit': 10, 'mean_acc': 0.9800000000000001}\n</code>\n</pre> <pre>\n<code>\n  0%|          | 0/2 [00:00&lt;?, ?it/s]\n 50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:01&lt;00:01,  1.68s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 5, 'limit': 3, 'mean_acc': 1.0}\n</code>\n</pre> <pre>\n<code>\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:03&lt;00:00,  1.65s/it]\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:14&lt;00:00,  3.57s/it]</code>\n</pre> <pre>\n<code>{'sampling_rate': 5, 'limit': 10, 'mean_acc': 0.99}\n</code>\n</pre> <pre>\n<code>\n</code>\n</pre> <pre><code>results = pd.DataFrame(results)\nresults\n</code></pre> sampling_rate limit mean_acc 0 1 3 0.90 1 1 10 0.83 2 2 3 1.00 3 2 10 0.97 4 3 3 1.00 5 3 10 0.98 6 5 3 1.00 7 5 10 0.99 <pre><code>\n</code></pre>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#setup-install-dependencies-imports-download-embeddings","title":"Setup: Install Dependencies, Imports &amp; Download Embeddings","text":""},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#code-walkthrough","title":"\ud83d\udc68\ud83c\udffe\u200d\ud83d\udcbb Code Walkthrough","text":"<p>Here's an explanation of the code structure provided:</p> <ol> <li>Loading Data: OpenAI embeddings are loaded from a parquet files (we can load upto 1M embedding) and concatenated into one array.</li> <li>Binary Conversion: A new array with the same shape is initialized with zeros, and the positive values in the original vectors are set to 1.</li> <li>Accuracy Function: The accuracy function compares original vectors with binary vectors for a given index, limit, and oversampling rate. The comparison is done using dot products and logical XOR, sorting the results, and measuring the intersection.</li> <li>Testing: The accuracy is tested for different oversampling rates (1, 2, 4), revealing a correctness of ~0.96 for an oversampling of 4.</li> </ol>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#loading-data","title":"\ud83d\udcbf Loading Data","text":""},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#accuracy-function","title":"\ud83c\udfaf Accuracy Function","text":"<p>We will use the accuracy function to compare the original vectors with the binary vectors for a given index, limit, and oversampling rate. The comparison is done using dot products and logical XOR, sorting the results, and measuring the intersection.</p>"},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#results","title":"\ud83d\udcca Results","text":""},{"location":"experimental/Binary%20Quantization%20from%20Scratch/#binary-conversion","title":"\u3193 Binary Conversion","text":"<p>Here, we will use 0 as the threshold for the binary conversion. All values greater than 0 will be set to 1, and others will remain 0. This is a simple and effective way to convert continuous values into binary values for OpenAI embeddings.</p>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/","title":"Binary Quantization with Qdrant","text":"<pre><code>!pip install qdrant-client pandas dataset --quiet --upgrade\n</code></pre> <pre><code>import os\nimport random\nimport time\n\nimport datasets\nimport numpy as np\nimport pandas as pd\nfrom qdrant_client import QdrantClient, models\n\nrandom.seed(37)\nnp.random.seed(37)\n</code></pre> <pre><code>dataset = datasets.load_dataset(\n    \"Qdrant/dbpedia-entities-openai3-text-embedding-3-small-1536-100K\", split=\"train\"\n)\nlen(dataset)\n</code></pre> <pre>\n<code>100000</code>\n</pre> <pre><code>n_dim = len(dataset[\"text-embedding-3-small-1536-embedding\"][0])\nn_dim\n</code></pre> <pre>\n<code>1536</code>\n</pre> <pre><code>client = QdrantClient(  # assumes Qdrant is launched at localhost:6333\n    prefer_grpc=True,\n)\n\ncollection_name = \"binary-quantization\"\n\nclient.create_collection(\n    collection_name=collection_name,\n    vectors_config=models.VectorParams(\n        size=n_dim,\n        distance=models.Distance.DOT,\n        on_disk=True,\n    ),\n    quantization_config=models.BinaryQuantization(\n        binary=models.BinaryQuantizationConfig(always_ram=True),\n    ),\n)\n</code></pre> <pre>\n<code>True</code>\n</pre> <pre><code>def iter_dataset(dataset):\n    for point in dataset:\n        yield point[\"openai\"], {\"text\": point[\"text\"]}\n\n\nvectors, payload = zip(*iter_dataset(dataset))\nclient.upload_collection(\n    collection_name=collection_name,\n    vectors=vectors,\n    payload=payload,\n    parallel=max(1, (os.cpu_count() // 2)),\n)\n</code></pre> <pre><code>collection_info = client.get_collection(collection_name=f\"{collection_name}\")\ncollection_info.dict()\n</code></pre> <pre>\n<code>{'status': &lt;CollectionStatus.GREEN: 'green'&gt;,\n 'optimizer_status': &lt;OptimizersStatusOneOf.OK: 'ok'&gt;,\n 'vectors_count': None,\n 'indexed_vectors_count': 97760,\n 'points_count': 100000,\n 'segments_count': 7,\n 'config': {'params': {'vectors': {'size': 1536,\n    'distance': &lt;Distance.DOT: 'Dot'&gt;,\n    'hnsw_config': None,\n    'quantization_config': None,\n    'on_disk': True,\n    'datatype': None},\n   'shard_number': 1,\n   'sharding_method': None,\n   'replication_factor': 1,\n   'write_consistency_factor': 1,\n   'read_fan_out_factor': None,\n   'on_disk_payload': True,\n   'sparse_vectors': None},\n  'hnsw_config': {'m': 16,\n   'ef_construct': 100,\n   'full_scan_threshold': 10000,\n   'max_indexing_threads': 0,\n   'on_disk': False,\n   'payload_m': None},\n  'optimizer_config': {'deleted_threshold': 0.2,\n   'vacuum_min_vector_number': 1000,\n   'default_segment_number': 0,\n   'max_segment_size': None,\n   'memmap_threshold': None,\n   'indexing_threshold': 20000,\n   'flush_interval_sec': 5,\n   'max_optimization_threads': None},\n  'wal_config': {'wal_capacity_mb': 32, 'wal_segments_ahead': 0},\n  'quantization_config': {'binary': {'always_ram': True}}},\n 'payload_schema': {}}</code>\n</pre> <pre><code>query_indices = random.sample(range(len(dataset)), 100)\nquery_dataset = dataset[query_indices]\nquery_indices\n</code></pre> <pre>\n<code>[89391,\n 79659,\n 12006,\n 80978,\n 87219,\n 97885,\n 83155,\n 67504,\n 4645,\n 82711,\n 48395,\n 57375,\n 69208,\n 14136,\n 89515,\n 59880,\n 78730,\n 36952,\n 49620,\n 96486,\n 55473,\n 58179,\n 18926,\n 6489,\n 11931,\n 54146,\n 9850,\n 71259,\n 37825,\n 47331,\n 84964,\n 92399,\n 56669,\n 77042,\n 73744,\n 47993,\n 83780,\n 92429,\n 75114,\n 4463,\n 69030,\n 81185,\n 27950,\n 66217,\n 54652,\n 8260,\n 1151,\n 993,\n 85954,\n 66863,\n 47303,\n 8992,\n 92688,\n 76030,\n 29472,\n 3077,\n 42454,\n 46120,\n 69140,\n 20877,\n 2844,\n 95423,\n 1770,\n 28568,\n 96448,\n 94227,\n 40837,\n 91684,\n 29785,\n 66936,\n 85121,\n 39546,\n 81910,\n 5514,\n 37068,\n 35731,\n 93990,\n 26685,\n 63076,\n 18762,\n 27922,\n 34916,\n 80976,\n 83189,\n 6328,\n 57508,\n 58860,\n 13758,\n 72976,\n 85030,\n 332,\n 34963,\n 85009,\n 31344,\n 11560,\n 58108,\n 85163,\n 17064,\n 44712,\n 45962]</code>\n</pre> <pre><code>## Add Gaussian noise to any vector\n\n\ndef add_noise(vector, noise=0.05):\n    return vector + noise * np.random.randn(*vector.shape)\n</code></pre> <pre><code>def correct(results, text):\n    return text in [x.payload[\"text\"] for x in results]\n\n\ndef count_correct(query_dataset, limit=1, oversampling=1, rescore=False):\n    correct_results = 0\n    for query_vector, text in zip(query_dataset[\"openai\"], query_dataset[\"text\"]):\n        results = client.search(\n            collection_name=collection_name,\n            query_vector=add_noise(np.array(query_vector)),\n            limit=limit,\n            search_params=models.SearchParams(\n                quantization=models.QuantizationSearchParams(\n                    rescore=rescore,\n                    oversampling=oversampling,\n                )\n            ),\n        )\n        correct_results += correct(results, text)\n    return correct_results\n</code></pre> <pre><code>limit_grid = [1, 3, 10, 20, 50]\noversampling_grid = [1.0, 3.0, 5.0]\nrescore_grid = [True, False]\nresults = []\n\nfor limit in limit_grid:\n    for oversampling in oversampling_grid:\n        for rescore in rescore_grid:\n            start = time.perf_counter()\n            correct_results = count_correct(\n                query_dataset, limit=limit, oversampling=oversampling, rescore=rescore\n            )\n            end = time.perf_counter()\n            results.append(\n                {\n                    \"limit\": limit,\n                    \"oversampling\": oversampling,\n                    \"bq_candidates\": int(oversampling * limit),\n                    \"rescore\": rescore,\n                    \"accuracy\": correct_results / 100,\n                    \"total queries\": len(query_dataset[\"text\"]),\n                    \"time\": end - start,\n                }\n            )\n</code></pre> <pre><code>df = pd.DataFrame(results)\n\ndf[[\"limit\", \"oversampling\", \"rescore\", \"accuracy\", \"bq_candidates\", \"time\"]]\n# df.to_csv(\"candidates-rescore-time.csv\", index=False)\n</code></pre> limit oversampling rescore accuracy bq_candidates time 0 1 1.0 True 0.95 1 0.300152 1 1 1.0 False 0.85 1 0.244668 2 1 3.0 True 0.95 3 0.124406 3 1 3.0 False 0.83 3 0.171471 4 1 5.0 True 0.98 5 0.118219 5 1 5.0 False 0.87 5 0.111914 6 3 1.0 True 0.95 3 0.121328 7 3 1.0 False 0.92 3 0.267725 8 3 3.0 True 0.96 9 0.416834 9 3 3.0 False 0.90 9 0.410730 10 3 5.0 True 0.97 15 0.231671 11 3 5.0 False 0.93 15 0.252269 12 10 1.0 True 0.96 10 0.133462 13 10 1.0 False 0.92 10 0.285158 14 10 3.0 True 0.95 30 0.320695 15 10 3.0 False 0.98 30 0.457904 16 10 5.0 True 0.96 50 0.453204 17 10 5.0 False 0.94 50 0.450944 18 20 1.0 True 0.97 20 0.361066 19 20 1.0 False 0.95 20 0.585992 20 20 3.0 True 0.96 60 0.550389 21 20 3.0 False 0.96 60 0.618630 22 20 5.0 True 1.00 100 0.458241 23 20 5.0 False 0.95 100 0.441106 24 50 1.0 True 0.98 50 0.603967 25 50 1.0 False 0.96 50 0.514531 26 50 3.0 True 1.00 150 0.548153 27 50 3.0 False 0.98 150 0.608930 28 50 5.0 True 1.00 250 0.487522 29 50 5.0 False 0.99 250 0.313810 <pre><code>\n</code></pre>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#binary-quantization-with-qdrant-openai-embedding","title":"Binary Quantization with Qdrant &amp; OpenAI Embedding","text":"<p>In the world of large-scale data retrieval and processing, efficiency is crucial. With the exponential growth of data, the ability to retrieve information quickly and accurately can significantly affect system performance. This blog post explores a technique known as binary quantization applied to OpenAI embeddings, demonstrating how it can enhance retrieval latency by 20x or more.</p>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#what-are-openai-embeddings","title":"What Are OpenAI Embeddings?","text":"<p>OpenAI embeddings are numerical representations of textual information. They transform text into a vector space where semantically similar texts are mapped close together. This mathematical representation enables computers to understand and process human language more effectively.</p>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#binary-quantization","title":"Binary Quantization","text":"<p>Binary quantization is a method which converts continuous numerical values into binary values (0 or 1). It simplifies the data structure, allowing faster computations. Here's a brief overview of the binary quantization process applied to OpenAI embeddings:</p> <ol> <li>Load Embeddings: OpenAI embeddings are loaded from parquet files.</li> <li>Binary Transformation: The continuous valued vectors are converted into binary form. Here, values greater than 0 are set to 1, and others remain 0.</li> <li>Comparison &amp; Retrieval: Binary vectors are used for comparison using logical XOR operations and other efficient algorithms.</li> </ol> <p>Binary Quantization is a promising approach to improve retrieval speeds and reduce memory footprint of vector search engines. In this notebook we will show how to use Qdrant to perform binary quantization of vectors and perform fast similarity search on the resulting index.</p>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Imports</li> <li>Download and Slice Dataset</li> <li>Create Qdrant Collection</li> <li>Indexing</li> <li>Search</li> </ol>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#1-imports","title":"1. Imports","text":""},{"location":"qdrant/Binary_Quantization_with_Qdrant/#2-download-and-slice-dataset","title":"2. Download and Slice Dataset","text":"<p>We will be using the dbpedia-entities dataset from the HuggingFace Datasets library. This contains 100K vectors of 1536 dimensions each</p>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#oversampling-vs-recall","title":"Oversampling vs Recall","text":""},{"location":"qdrant/Binary_Quantization_with_Qdrant/#preparing-a-query-dataset","title":"Preparing a query dataset","text":"<p>For the purpose of this illustration, we'll take a few vectors which we know are already in the index and query them. We should get the same vectors back as results from the Qdrant index. </p>"},{"location":"qdrant/Binary_Quantization_with_Qdrant/#why-results-for-oversampling10-and-limit1-with-rescoretrue-are-better-than-with-rescorefalse","title":"Why results for oversampling=1.0 and limit=1 with rescore=True are better than with rescore=False?","text":"<p>It might seem that with oversampling=1.0 and limit=1 Qdrant retrieves only 1 point, and it does not matter whether we rescore it or not, it should stay the same, but with a different score (from original vectors).</p> <p>But in fact, there are 2 reasons why results are different: 1) HNSW is an approximate algorithm, and it might return different results for the same query.  2) Qdrant stores points in segments. When we do a query for 1 point, Qdrant looks for this one point in each segment, and then chooses the best match.  3) In this example we had 8 segments, Qdrant found 8 points with binary scores, replaced their scores with original vectors scores, and selected the best one from them, which led to a better accuracy. </p>"},{"location":"qdrant/Retrieval_with_FastEmbed/","title":"Retrieval with FastEmbed","text":"<pre><code># !pip install fastembed --quiet --upgrade\n</code></pre> <p>Importing the necessary libraries:</p> <pre><code>import numpy as np\nfrom fastembed import TextEmbedding\n</code></pre> <pre><code># Example list of documents\ndocuments: list[str] = [\n    \"Maharana Pratap was a Rajput warrior king from Mewar\",\n    \"He fought against the Mughal Empire led by Akbar\",\n    \"The Battle of Haldighati in 1576 was his most famous battle\",\n    \"He refused to submit to Akbar and continued guerrilla warfare\",\n    \"His capital was Chittorgarh, which he lost to the Mughals\",\n    \"He died in 1597 at the age of 57\",\n    \"Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\",\n    \"His legacy is celebrated in Rajasthan through festivals and monuments\",\n    \"He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\",\n    \"His life has been depicted in various films, TV shows, and books\",\n]\n# Initialize the DefaultEmbedding class with the desired parameters\nembedding_model = TextEmbedding(model_name=\"BAAI/bge-small-en\")\n\n# We'll use the passage_embed method to get the embeddings for the documents\nembeddings: list[np.ndarray] = list(\n    embedding_model.passage_embed(documents)\n)  # notice that we are casting the generator to a list\n\nprint(embeddings[0].shape, len(embeddings))\n</code></pre> <pre>\n<code>(384,) 10\n</code>\n</pre> <pre><code>query = \"Who was Maharana Pratap?\"\nquery_embedding = list(embedding_model.query_embed(query))[0]\nplain_query_embedding = list(embedding_model.embed(query))[0]\n\n\ndef print_top_k(query_embedding, embeddings, documents, k=5):\n    # use numpy to calculate the cosine similarity between the query and the documents\n    scores = np.dot(embeddings, query_embedding)\n    # sort the scores in descending order\n    sorted_scores = np.argsort(scores)[::-1]\n    # print the top 5\n    for i in range(k):\n        print(f\"Rank {i+1}: {documents[sorted_scores[i]]}\")\n</code></pre> <pre><code>query_embedding[:5], plain_query_embedding[:5]\n</code></pre> <pre>\n<code>(array([-0.06002192,  0.04322132, -0.00545516, -0.04419701, -0.00542277],\n       dtype=float32),\n array([-0.06002192,  0.04322132, -0.00545516, -0.04419701, -0.00542277],\n       dtype=float32))</code>\n</pre> <p>The <code>query_embed</code> is specifically designed for queries, leading to more relevant and context-aware results. The retrieved documents tend to align closely with the query's intent.</p> <p>In contrast, <code>embed</code> is a more general-purpose representation that might not capture the nuances of the query as effectively. The retrieved documents using plain embeddings might be less relevant or ordered differently compared to the results obtained using query embeddings.</p> <p>Conclusion: Using query and passage embeddings leads to more relevant and context-aware results.</p>"},{"location":"qdrant/Retrieval_with_FastEmbed/#retrieval-with-fastembed","title":"\u2693\ufe0f Retrieval with FastEmbed","text":"<p>This notebook demonstrates how to use FastEmbed to perform vector search and retrieval. It consists of the following sections:</p> <ol> <li>Setup: Installing the necessary packages.</li> <li>Importing Libraries: Importing FastEmbed and other libraries.</li> <li>Data Preparation: Example data and embedding generation.</li> <li>Querying: Defining a function to search documents based on a query.</li> <li>Running Queries: Running example queries.</li> </ol>"},{"location":"qdrant/Retrieval_with_FastEmbed/#setup","title":"Setup","text":"<p>First, we need to install the dependencies. <code>fastembed</code> to create embeddings and perform retrieval.</p>"},{"location":"qdrant/Retrieval_with_FastEmbed/#data-preparation","title":"Data Preparation","text":"<p>We initialize the embedding model and generate embeddings for the documents.</p>"},{"location":"qdrant/Retrieval_with_FastEmbed/#tip-prefer-using-query_embed-for-queries-and-passage_embed-for-documents","title":"\ud83d\udca1 Tip: Prefer using <code>query_embed</code> for queries and <code>passage_embed</code> for documents.","text":""},{"location":"qdrant/Retrieval_with_FastEmbed/#querying","title":"Querying","text":"<p>We'll define a function to print the top k documents based on a query, and prepare a sample query.</p>"},{"location":"qdrant/Usage_With_Qdrant/","title":"Usage With Qdrant","text":"<pre><code>!pip install 'qdrant-client[fastembed]' --quiet --upgrade\n</code></pre> <p>Importing the necessary libraries:</p> <pre><code>from qdrant_client import QdrantClient\n</code></pre> <pre><code># Example list of documents\ndocuments: list[str] = [\n    \"Maharana Pratap was a Rajput warrior king from Mewar\",\n    \"He fought against the Mughal Empire led by Akbar\",\n    \"The Battle of Haldighati in 1576 was his most famous battle\",\n    \"He refused to submit to Akbar and continued guerrilla warfare\",\n    \"His capital was Chittorgarh, which he lost to the Mughals\",\n    \"He died in 1597 at the age of 57\",\n    \"Maharana Pratap is considered a symbol of Rajput resistance against foreign rule\",\n    \"His legacy is celebrated in Rajasthan through festivals and monuments\",\n    \"He had 11 wives and 17 sons, including Amar Singh I who succeeded him as ruler of Mewar\",\n    \"His life has been depicted in various films, TV shows, and books\",\n]\n</code></pre> <p>This tutorial demonstrates how to utilize the QdrantClient to add documents to a collection and query the collection for relevant documents.</p> <pre><code>client = QdrantClient(\":memory:\")\nclient.add(collection_name=\"test_collection\", documents=documents)\n</code></pre> <pre>\n<code>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 77.7M/77.7M [00:05&lt;00:00, 14.6MiB/s]\n</code>\n</pre> <pre>\n<code>['4fa8b10c78da4b18ba0830ba8a57367a',\n '2eae04b515ee4e9185a9a0e6be812bba',\n 'c6039f88486f47f1835ae3b069c5823c',\n 'c2c8c51e305144d1917b373125fb4d95',\n '79fd23b9ec0648cdab38d1947c6b933e',\n '036aa200d8c3492b8a438e4f825f5e7f',\n 'c35c77f3ea37460a9a13723fb77b7367',\n '6ebccbca571b40d0ab6e83e5e0f2f562',\n '38048c2ccc1d4962a4f8f1bd89c8357a',\n 'c6b09308360140c7b4f106af3658a31e']</code>\n</pre> <p>These are the ids of the documents we just added. We don't have a use for them in this tutorial, but they can be used to update or delete documents.</p> <pre><code># Prepare your documents, metadata, and IDs\ndocs = [\"Qdrant has Langchain integrations\", \"Qdrant also has Llama Index integrations\"]\nmetadata = [\n    {\"source\": \"Langchain-docs\"},\n    {\"source\": \"Linkedin-docs\"},\n]\nids = [42, 2]\n\n# Use the new add method\nclient.add(collection_name=\"demo_collection\", documents=docs, metadata=metadata, ids=ids)\n</code></pre> <pre>\n<code>[42, 2]</code>\n</pre> <p>Behind the scenes, Qdrant Client uses the FastEmbed library to make a passage embedding and then uses the Qdrant API to upsert the documents with metadata, put together as a Points into the collection.</p> <pre><code>search_result = client.query(\n    collection_name=\"demo_collection\", query_text=\"This is a query document\"\n)\nprint(search_result)\n</code></pre> <pre>\n<code>[QueryResponse(id=42, embedding=None, metadata={'document': 'Qdrant has Langchain integrations', 'source': 'Langchain-docs'}, document='Qdrant has Langchain integrations', score=0.8276550115796268), QueryResponse(id=2, embedding=None, metadata={'document': 'Qdrant also has Llama Index integrations', 'source': 'Linkedin-docs'}, document='Qdrant also has Llama Index integrations', score=0.8265536935180283)]\n</code>\n</pre>"},{"location":"qdrant/Usage_With_Qdrant/#usage-with-qdrant","title":"Usage With Qdrant","text":"<p>This notebook demonstrates how to use FastEmbed and Qdrant to perform vector search and retrieval. Qdrant is an open-source vector similarity search engine that is used to store, organize, and query collections of high-dimensional vectors. </p> <p>We will use the Qdrant to add a collection of documents to the engine and then query the collection to retrieve the most relevant documents.</p> <p>It consists of the following sections:</p> <ol> <li>Setup: Installing necessary packages, including the Qdrant Client and FastEmbed.</li> <li>Importing Libraries: Importing FastEmbed and other libraries</li> <li>Data Preparation: Example data and embedding generation</li> <li>Querying: Defining a function to search documents based on a query</li> <li>Running Queries: Running example queries</li> </ol>"},{"location":"qdrant/Usage_With_Qdrant/#setup","title":"Setup","text":"<p>First, we need to install the dependencies. <code>fastembed</code> to create embeddings and perform retrieval, and <code>qdrant-client</code> to interact with the Qdrant database.</p>"},{"location":"qdrant/Usage_With_Qdrant/#data-preparation","title":"Data Preparation","text":"<p>We initialize the embedding model and generate embeddings for the documents.</p>"},{"location":"qdrant/Usage_With_Qdrant/#tip-prefer-using-query_embed-for-queries-and-passage_embed-for-documents","title":"\ud83d\udca1 Tip: Prefer using <code>query_embed</code> for queries and <code>passage_embed</code> for documents.","text":""},{"location":"qdrant/Usage_With_Qdrant/#adding-documents","title":"\u2795 Adding Documents","text":"<p>The <code>add</code> creates a collection if it does not already exist. Now, we can add the documents to the collection:</p>"},{"location":"qdrant/Usage_With_Qdrant/#running-queries","title":"\ud83d\udcdd Running Queries","text":"<p>We'll define a function to print the top k documents based on a query, and prepare a sample query.</p>"},{"location":"qdrant/Usage_With_Qdrant/#conclusion","title":"\ud83c\udfac Conclusion","text":"<p>This tutorial demonstrates the basics of working with the QdrantClient to add and query documents. By following this guide, you can easily integrate Qdrant into your projects for vector similarity search and retrieval.</p> <p>Remember to properly handle the closing of the client connection and further customization of the query parameters according to your specific needs.</p> <p>The official Qdrant Python client documentation can be found here for more details on customization and advanced features.</p>"}]}